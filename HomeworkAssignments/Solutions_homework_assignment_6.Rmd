---
title: "STATS 205: Homework Assignment 6 (Spring 2019)"
author: ""
date: "5/20/2019"
output:
  pdf_document: 
    includes:
      in_header: header.tex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

Solve problems 1-2 and 4 from the textbook **HWC** [available here](https://onlinelibrary.wiley.com/doi/book/10.1002/9781119196037) and problem 3 from **W 2006**.


\blc

1) **HWC** Page 661, Problem 1 (local averaging). \rc  [4 points] \bc

The data set `cars` from Ezekiel (1930) contains stopping distances for various speeds. Smooth the data using Friedman’s smoother by choosing your own value of the span. Use dist as the dependent (response) variable and speed as the independent (predictor) variable. Using trial and error, what seems to be a reasonable span? Comment on the graphical comparison between the estimate using your choice of span with the estimate using the span chosen by cross-validation.

\bc 

```{r}
library(datasets)
data(cars)
head(cars)
```



Let's choose **tweeter (span = .05)**, **midrange (span = .2)**, **woofer(.5)** values as candidate spans.

Then, we wil decide span between tweeter/midrange/woofer.
```{r}
library(RColorBrewer)
library(wesanderson)
library(ggplot2)
col.span = c(wes_palette("Cavalcanti1", n = 5), wes_palette("Darjeeling1", n = 5))

plot.with.diff.span = function(sp, col){
  fit = supsmu(cars$speed, cars$dist, span = sp)
  dfi = data.frame(x = fit$x, fit.span = fit$y)
  p = ggplot() + 
  geom_point(data = cars, aes(x = speed, y = dist)) +
    geom_line(data = dfi, 
    aes(x = x, y = fit.span), color = col) + 
    ggtitle(paste0("Span is ", sp)) + 
    theme(plot.title = element_text(hjust = 0.5))
  return(p)
}

```

```{r}
plot.with.diff.span(.05, col.span[1])
```

tweeter span = .01 gives an undersmooth estimate (excess variability variability in the smoothed estimate).

```{r}
plot.with.diff.span(.2, col.span[2])
```


Mid-range span = .2 gives reasonable estimate in that it is neither oversmoothing nor too irregular.

```{r}
plot.with.diff.span(.5, col.span[3])
```



Woofer span = .5 gives an oversmooth estimate (biased). 

Testing smooth estimate with tweeter, midrange, and woofer span values, we can choose span .2.

```{r}
plot.with.diff.span(.2, col.span[4])
```


Now we will use cross-validation.

```{r}
library(NSM3)
Friedman.fit.cv = supsmu(cars$speed, 
  cars$dist, span = "cv")
df = data.frame(speed.x = Friedman.fit.cv$x, 
  Friedman.fit.cv = Friedman.fit.cv$y)
library(dplyr)
library(ggplot2)
p = ggplot() + 
  geom_point(data = cars, aes(x = speed, y = dist)) +
  geom_line(data = df, 
    aes(x = speed.x, y = Friedman.fit.cv), color = "red")
```


Let's plot CV-based smooth estimate and smooth estimate with manually selected span .2.

```{r}
plot.with.diff.span(.2, col.span[4]) + 
  geom_line(data = df, aes(x = speed.x, y = Friedman.fit.cv), color = "red") + ggtitle("")
```

We can observe that estimates using CV and manual span (mid-range) are similar.

\blc

2) **HWC** Page 661, Problem 2 (local averaging and choosing optimal span). \rc  [4 points] \bc

Consider the data set sunspots from Andrews and Herzberg (1985) as a response variable. For the predictor data $x$, use
```{r}
data("sunspots")
x = c(1:length(sunspots))
```

Apply Friedman’s smoother using trial and error to find a span that seems to work well with the data. Then find an estimate using the span determined by cross-validation. Describe the results (taking into account Comment 5).
```{r}
y = sunspots
```

\bc
Let's consider tweeter, mid-range, and woofer spans.

```{r}
col.span = c(wes_palette("Cavalcanti1", n = 5), wes_palette("Darjeeling1", n = 5))
df.sunspot= data.frame(x, y)
plot.with.diff.span.sunspot = function(sp, col){
  fit = supsmu(df.sunspot$x, df.sunspot$y, span = sp)
  dfi = data.frame(x = fit$x, fit.span = fit$y)
  p = ggplot() + 
  geom_point(data = df.sunspot, aes(x = x, y = y)) +
    geom_line(data = dfi, 
    aes(x = x, y = fit.span), color = col) + 
    ggtitle(paste0("Span is ", sp)) + 
    theme(plot.title = element_text(hjust = 0.5))
  return(p)
}

```

```{r}
plot.with.diff.span.sunspot(.05, col.span[1])
```

Estimate is undersmooth with span .05 (tweeter) (excessive variability in the estimate) and didn't pick up the inflection in the data.

```{r}
plot.with.diff.span.sunspot(.2, col.span[2])
```


The estimator is too smooth (biased) wiht span .2. and didn't pick up the inflection in the data.

We don't need to fit with span .5 (woofer) because it'll be too smooth than with span .2. Now, we need a span value less than .05. According to **HWC** page 661, comment 5, in order to pick the inflection in the data, we need a span value less than tweeter (.05). Let's choose span .01.

```{r}
plot.with.diff.span.sunspot(.01, col.span[3])
```

Span .01 gives reasonable estimate.

\blc
3) **W 2006** Page 121, Problem 3. \rc  [4 points] \bc

Download and save `forensic_glass_data_W2006.dat` available at [Canvas @ Stanford](https://canvas.stanford.edu/courses/99104/files/folder/Data) 

```{r}
forensic.data = read.table("forensic_glass_data_W2006.dat", header = TRUE)
head(forensic.data)
```

Let $Y$ be refractive index and let $x$ be aluminium content (the fourth variable). 
```{r}
library(dplyr)
Y = select(forensic.data, RI)
x = select(forensic.data, Al)
Y = Y[,1]
x = x[,1]
df  = data.frame(x = x, y = Y)
```

(1) Perform a nonparametric regression to fit the model $Y = r\left(x\right)+\epsilon$. Use the following estimators: 
    (i) regressogram, \rc [4 points] \bc
    (ii) kernel, \rc [4 points] \bc
    (iii) local linear kernel regression, \rc [4 points] \bc
    (iv) spline. \rc [4 points] \blc

In each case, use cross-validation to choose the amount of smoothing. 

(2) Estimate the variance. 

(3) Construct 95 percent confidence bands for your estimates.


\bc

(i) regressogram 

```{r include=FALSE, eval=FALSE}
df.order.x = df[order(df$x, decreasing = FALSE), ]
bin.grid = seq(1, 15)
library(HoRM)
reg.fit = list()
for(i in 1:length(bin.grid)){
  reg.fit[[i]] = regressogram(df.order.x$x, df.order.x$y, nbins = bin.grid[i])
}

compute.aMSE = function(nbins, df){# df should be ordered by x variable
  nbins = nbins
  xmin = min(df$x)
  xmax = max(df$x)
  binWidth = (xmax - xmin) / nbins
  errors = list()
      for(i in 0:(nbins-1)){
        binStart = xmin + i*binWidth
        binEnd = xmin + i*binWidth + binWidth
      
        yValues = list()
        binX = list()
        counter = 0
        for(j in df$x[1:length(df$x)]){
          if (j > binStart && j < binEnd){
            yValues = append(yValues, df$y[counter])
            binX = append(binX, j)
          }
          counter = counter + 1
        }
        average = mean(unlist(yValues))
        for(j in binX){
          errors = append(errors, (average - df$y[which(df$x == j)])^2)
        }
      }
  aMSE = mean(unlist(errors))
  return(aMSE)
}

aMSE = lapply(as.list(bin.grid), function(x){
  compute.aMSE(x, df.order.x)
})

aMSE = unlist(aMSE)
plot(x = bin.grid, y = aMSE, type ="l")

#The aMSE is minimized at number of bins 9.
```



(ii) kernel
```{r warning=FALSE, message=FALSE}
library(np)
kernel.fit = npreg(x, Y, ckertype="epanechnikov", bwmethod = "cv.ls")
summary(kernel.fit)
```

The variance of residula is $2.5^{2} = 6.25$.
```{r message=FALSE, warning=FALSE}
resample.data = function(df) { 
  sample.rows = sample(1:nrow(df), replace = TRUE) 
  return(df[sample.rows,]) 
}

npr.nox.on.E = function(df.star) {
  fit = npreg(df.star$x, df.star$y, ckertype="epanechnikov", bwmethod = "cv.ls", verbose = FALSE)
  return(fit) 
}

evaluation.points = seq((min(df$x) -.1), (max(df$x)+.1), by =.01)

eval.npr = function(npr) { 
  return(predict(npr,
     exdat = evaluation.points))
}

npr.cis = function(B,alpha, df, obs.curve) { 
  tboot= replicate(B, eval.npr(npr.nox.on.E(resample.data(df))))
  low.quantiles = apply(tboot, 1, quantile, probs = alpha/2)   
  high.quantiles = apply(tboot, 1, quantile, probs = (1-alpha/2))
  low.cis = 2*obs.curve - high.quantiles 
  high.cis = 2*obs.curve - low.quantiles 
  cis = rbind(low.cis, high.cis) 
  return(list(cis=cis, tboot= t(tboot)))
}

npr.on.obs.data = npr.nox.on.E(df)
obs.curve = eval.npr(npr.on.obs.data)

npr.cis.eval = npr.cis(B = 100, alpha = 0.05,
df = df, obs.curve = obs.curve)


df.plot.ci = data.frame(x = evaluation.points, obs.curve = obs.curve, low.cis = npr.cis.eval$cis[1,], upper.cis = npr.cis.eval$cis[2,])

p = ggplot() + 
  geom_point(data = df, aes(x = x, y = y)) +
  geom_line(data = df.plot.ci, aes(x = evaluation.points, y = low.cis), color = "red", linetype = "dashed", size = 1)

p + geom_line(data = df.plot.ci, aes(x = evaluation.points, y = upper.cis), color = "red", linetype = "dashed", size = 1) +
  geom_line(data = df.plot.ci, aes(x = evaluation.points, y = obs.curve), color = "blue", size = 1)
```


(iii) local linear kernel regression

Similar approach as above with the following R function:
```{r eval = FALSE, message=FALSE, warning=FALSE}
local.lin.smooth.fit = npreg(df$x, df$y, ckertype="epanechnikov", bwmethod = "cv.ls", regtype="ll")
```

(iv) spline

Similar approach as above with the following R function:
```{r}
spline.fit = smooth.spline(df$x, df$y, cv = FALSE) # using generalized’ cross-validation 
spline.fit
```

\blc

4) **HWC** Page 644 Problem 6 (a)  (Wavelet).  \rc  [4 points] \bc
Let $y$ be the first 512 components in the sunspots data from package datasets. 

(a) Use the `mra` command to plot $f_{4}$, $f_{5}$ and $f_{6}$ using the Haar wavelet. Describe the different characteristics of each of these three smooth approximations. For example, use $J =5$ and either $[[6]]$ or \$S5  to find $f_{4}$. 

\bc

```{r}
library(waveslim)
data("sunspots") 
n = 2^9#512
x = (seq(1, n, by =1)-1)/n
y = sunspots[1:512]
dw.fit =mra(y, method="dwt", wf="haar", J=5)
```

`dw.fit` will give list of 6 vectors.

```{r}
f4 = dw.fit[[6]]
f5 = dw.fit[[6]] + dw.fit[[5]]
f6 = dw.fit[[6]] + dw.fit[[5]] + dw.fit[[4]] 
df = data.frame(x = x, y = y, f4 = f4, f5 = f5, f6 = f6)
```

```{r}
ggplot() + geom_point(data = df, aes(x = x, y = y)) + geom_line(data = df ,aes(x = x, y = f4), color = "blue") + geom_line(data = df,aes(x = x, y = f5), color = "red") + geom_line(data = df,aes(x = x, y = f6), color = "green")
```

The maximum resolution level is $J =9$, but we use $J=5$. Thus, we decompose the sampled data into fewer levels.

We obtain approximations of the sampled data $y$ at various resolution levels. Comparing $f_{4},f_{5},f_{6}$, $f_{4}$ is the approxiamtion at the least resolution level and f_{6} is the approximation at the highest resolution level. $f_{4}$ is too smooth than $f_{6}$.