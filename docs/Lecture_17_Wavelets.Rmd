---
title: "Lecture 17: Wavelets"
shorttitle: "STATS 205 Lecture 17"
author: "Pratheepa Jeganathan"
date: "05/10/2019"
output: 
  beamer_presentation:
    colortheme: "seahorse"
    slide_level: 2
    includes:
      in_header: header.tex
bibliography: nonparamterics.bib

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width = 5, fig.height = 5, fig.align="center",message=FALSE, warning=FALSE, out.width = '70%')
```

#  Recall

##
- One sample sign test, Wilcoxon signed rank test, large-sample approximation, median, Hodges-Lehman estimator, distribution-free confidence interval.
- Jackknife for bias and standard error of an estimator.
- Bootstrap samples, bootstrap replicates.
- Bootstrap standard error of an estimator.
- Bootstrap percentile confidence interval.
- Hypothesis testing with the bootstrap (one-sample problem.)
- Assessing the error in bootstrap estimates.
- Example: inference on ratio of heart attack rates in the aspirin-intake group to the placebo group.
- The exhaustive bootstrap distribution.


##
- Discrete data problems (one-sample, two-sample proportion tests, test of homogeneity, test of independence).
- Two-sample problems (location problem - equal variance, unequal variance, exact test or Monte Carlo, large-sample approximation, H-L estimator, dispersion problem, general distribution).
- Permutation tests (permutation test for continuous data, different test statistic, accuracy of permutation tests).
- Permutation tests (discrete data problems, exchangeability.)
- Rank-based correlation analysis (Kendall and Spearman correlation coefficients.)
- Rank-based regression (straight line, multiple linear regression, statistical inference about the unknown parameters, nonparametric procedures - does not depend on the distribution of error term.)
- Smoothing (density estimation, bias-variance trade-off, curse of dimensionality)
- Nonparametric regression (Local averaging, local regression, kernel smoothing, local polynomial, penalized regression)

## 
- Cross-validation, Variance Estimation, Confidence Bands, Bootstrap Confidence Bands.
 

# Wavelets

# Spatially inhomogeneous functions

## Example
- Doppler function
```{r fig.show='hide'}
library(ggplot2)
r = function(x){
  sqrt(x*(1-x))*sin(2.1*pi/(x+.05))
}
ep = rnorm(1000)
y = r(seq(1, 1000, by = 1)/1000) + .1 * ep
df = data.frame(x = seq(1, 1000, by = 1)/1000, y = y)
ggplot(df) +
  geom_point(aes(x = x, y = y))
```


## Example
```{r echo=FALSE}
ggplot(df) +
  geom_point(aes(x = x, y = y))
```

## Example

- Doppler function is spatially inhomogeneous (smoothness varies over $x$).
- Estimate by local linear regression

```{r fig.show='hide'}
library(np)
doppler.npreg <- npreg(bws=.005, 
  txdat=df$x,
     tydat=df$y, 
  ckertype="epanechnikov")

doppler.npreg.fit = data.frame(x = df$x, 
  y = df$y, 
  kernel.fit = fitted(doppler.npreg))

p = ggplot(doppler.npreg.fit) + 
  geom_point(aes(x = x, y = y)) + 
  geom_line(aes(x = x, y= kernel.fit), color = "red")  
```

## Example

```{r echo=FALSE}
p 
```

## Example

- Doppler function fit using local linear regression.
    - Effective degrees of freedom 166.
    - Fitted function is very wiggly.
    - If we smooth more, right-hand side of the fit would look better at the cost of missing structure near $x = 0$.
    
# Introduction

## 

- Construct basis functions that are 
  - multiscale.
  - spatially/ locally adaptive.
- Find sparse set of coefficients for a given basis.

## 

- Function $f$ belongs to a class of functions $\mathcal{F}$ possessing more general characteristics, such as a certain level of smoothness.
- Estimate $f$ by representing the function in another domain.
- Use an orthogonal series representation of the function $f$.
- Estimating a set of scalar coefficients that represent $f$ in the orthogonal series domain.
- Tool: Wavelets
    - ability to estimate both global and local features in the underlying function
    
# Sparseness

##

- **W 2006** Chapter 9
- A function $f = \sum_{j}\beta_{j}\phi_{j}$ is sparse in a basis $\phi_{1}, \phi_{2}, \cdots$ if most of the $\beta_{j}$'s are zero.
- Sparseness generalizes smoothness: smooth functions are sparse but there are also non smooth functions that are sparse.
- Sparseness is not captured by $L_{2}$ norm.
    - Example $\va = \left(1, 0, \cdots, 0 \right)$ and $\vb = \left(1/\sqrt{n}, 1/\sqrt{n}, \cdots, 1/\sqrt{n} \right)$.
    - $\va$ is sparse.
    - $L_{2}$ norms are $\left| \left| \va\right| \right|_{2} = \left| \left| \vb\right| \right|_{2} = 1$.
    - $L_{1}$ norms are $\left| \va \right|_{1} = 1$ and $\left|  \vb \right|_{1} = \sqrt{n}$.
    
# Wavelets

##
- Data: There are $n$ pairs of observations $\left(x_{1}, Y_{1}\right), \left(x_{2}, Y_{2}\right), \cdots, \left(x_{n}, Y_{n}\right)$.
- Assumptions
    - $Y_{i} = f\left(x_{i}\right) + \epsilon_{i}.$
    - $\epsilon_{i}$ are IID.
    - $\int f^{2} < \infty$ and $f$ is defined on a close interval $\left[ a,b\right]$. For simplicity, we will consider $\left[ a,b\right] = \left[0,1\right]$.

# Wavelet representation of a function

## Basis functions
-   $\vpsi = \lbrace \psi_{1}, \psi_{2}, \cdots \rbrace$ is called a basis for a class of functions $\mathcal{F}$. Then, for $f \in \mathcal{F}$, $$f\left(x\right) = \sum_{i=1}^{\infty}\theta_{i}\psi_{i}\left(x\right).$$
    - $\theta_{i}$- scalar constants/coefficients
    - Basis functions are orthogonal if $<\psi_{i}, \psi_{j}> = 0$ for $i \neq j$.
    - If basis functions are orthonormal, they are orthogonal and $<\psi_{i}, \psi_{i}> = 1$.

- \rc How do we construct basis functions $\psi_{i}$'s ? \bc 


## Basis functions
- If $\psi$ is a wavelet function, then the collection of functions $$\vPsi = \lbrace \psi_{ij}; j. k \hspace{.1in} \text{integers} \rbrace,$$ where $$\psi_{ij} = 2^{j/2}\psi\left(2^{j}x - k\right),$$ forms a basis for square integrable functions.
- $\vPsi$ is a collection of translation (shift) and dilation (scaling) of $\psi$.
    - $\psi$ can be defined in any range of real line.
    - $\int \psi = 1$
        - value of $\psi$ is near 0 except over a small range.

## Some examples for wavelets
- Haar wavelets (1910)

```{r echo=FALSE}
library(png)
library(grid)
img <- readPNG("haar_wavelet.png")
grid.raster(img)
```

## Some examples for wavelets
- Daubechies wavelets (1992)

```{r echo=FALSE}
library(png)
library(grid)
img <- readPNG("diab_wavelet.png")
grid.raster(img)
```

## Multiresolution analysis (MRA)
- Carefully construct wavelet function $\psi$.
- MRA: interpretation of the wavelet representation of $f$ in terms of location and scale. 
- Translation and dilation of $\psi$ gives $$f \left(x\right)= \sum_{j \in \mathcal{Z}} \sum_{k \in \mathcal{Z}}\theta_{jk} \psi \left(x\right),$$ where $\mathcal{Z}$ is a set of integers.
    - scale - frequency.
    - For fixed $j$, $k$ represents the behavior of $f$ at resolution scale $j$ and a particular location.
    - function $f$ at differing resolution (scale, frequency) levels $j$ and locations $k$ - MRA.
    
## Multiresolution analysis (MRA)
- Cumulative approximation of $f$ using $j < J,$ $$f _{J}\left(x\right) = \sum_{j < J} \sum_{k \in \mathcal{Z}}\theta_{jk} \psi \left(x\right).$$
    - $J$ increases - $f_{J}$ models smaller scales (higher frequency) of $f$ - changes occur in the small interval of $x$.
    - $J$ decreases - $f_{J}$ models larger scale (lower frequency) behavior of $f$.
- A complete representation of $f$ is the limit of $f_{J}$.

## Multiresolution analysis (MRA)
- Write $f _{J}\left(x\right)$ as follows: $$f _{J}\left(x\right) = \sum_{k \in \mathcal{Z}}\xi_{j0}\phi_{j0 k}\left(x\right) + \sum_{j0 \leq j < J}\sum_{k \in \mathcal{Z}}\theta_{jk}\psi_{jk}\left(x\right),$$ where $f_{j0} = \sum_{k \in \mathcal{Z}}\xi_{j0}\phi_{j0 k}\left(x\right).$
    - Add second term to $f_{j0}$ allows for modeling higher scale-frequency behavior of $f$.
    - $f_{j0}$ approximation at the smooth resolution level.
    - Each of the remaining resolution level series is a “detail” level.
    - $\phi$ - scaling function (Father wavelet).
    - $\psi$ - wavelet function (Mother wavelet).
  
## MRA Using the Haar Wavelet (Example)
- Approximate $f(x) = x, x \in \left(0,1 \right)$.
- Define Haar wavelet function 
\begin{equation}
\psi \left(x\right) =
 \begin{cases}
1 &  x \in [0, 1/2),\\
-1 & x \in [1/2, 1),
 \end{cases}
\end{equation}
and \begin{equation}\phi  \left(x\right) = 1, x \in [0,1].\end{equation}
- Haar wavelet allows exact determination of the wavelet coefficients $\theta_{jk}.$

##
- Source **HWC**
```{r echo=FALSE}
library(png)
library(grid)
img <- readPNG("Harrexample.png")
grid.raster(img)
```

## MRA Using the D2 wavelet
- Source **HWC**
```{r echo=FALSE}
library(png)
library(grid)
img <- readPNG("D2_wavelet.png")
grid.raster(img)
```

## MRA Using the D2 wavelet

- To avoid boundary issues using D2
    - Specify using reflection at the boundaries, rather than periodicity.
    - increase the number of indices $k$ that must be considered at each resolution level $j$.
    
## Discrete wavelet transform
- Cascade algorithm provides MRA (Mallat 1989).
- Some restrictions
    - $J = \text{log}_{2}\left(n\right).$
    - The number of resolution levels in the wavelet series is truncated both above and below in practice, resulting in $J - j0 +1$ series, each representing a resolution level.
-  Commands in R that make use of the DWT are `dwt`, `idwt`, and `mra` in package `waveslim` (Whitcher (2010)).

## Discrete wavelet transform (Example)
- $y_{i} = x_{i} = \left(i -1 \right)/n,  i =1, 2, \cdots, n.$

```{r}
n = 2^12
xi  = (seq(1, n, by =1) - 1)/n
yi = xi
library(waveslim)
```

##
- Haar basis.
- Number of resolution levels $J =12$.
- Decompose the sample data $y$.
```{r}
dwt.fit = mra(yi, method="dwt", wf="haar", J=12)
```
- Output is a list of 13 vectors.
- The first vector is the change necessary to go from the approximation $f_{12}$ to $f_{13}$- approximation at the highest detail resolution level.
- The next to last vector is $f_{1} -f_{0}$.
- The final, thirteenth vector is the smooth approximation $f_{0}$.
- Summing the thirteenth vector and the twelfth vector results in $f_{1}$.
- $f_{13} -f_{12}, f_{12} -f_{11}, \cdots, f_{1} -f_{0}, f_{0}.$

##
```{r fig.show='hide'}
f0 = dwt.fit[[13]]
f1 = dwt.fit[[13]]+dwt.fit[[12]]
df = data.frame(x = xi, y = yi, 
  f0=f0, f1 = f1)
p1 = ggplot() + 
  geom_point(data  = df , 
    aes(x = xi, y = yi))+
  geom_line(data  = df , 
    aes(x = xi, y = f0), color = "blue") +
  geom_line(data = df, 
    aes(x = xi, y = f1), color = "red")
```


##
```{r echo=FALSE}
p1 + ggtitle("Wavelet representation with resolution J = 0 and 1") + theme(plot.title = element_text(hjust = 0.5))
```


##
```{r fig.show='hide'}
f2 = dwt.fit[[13]]+dwt.fit[[12]] + dwt.fit[[11]]
df = data.frame(x = xi, y = yi, 
  f0 = f0, f1 = f1, f2 = f2)
p2 = ggplot() + 
  geom_point(data  = df , 
    aes(x = xi, y = yi)) +
  geom_line(data = df, 
    aes(x = xi, y = f0), color = "blue")+
  geom_line(data = df, 
    aes(x = xi, y = f1), color = "red")+
  geom_line(data = df, 
    aes(x = xi, y = f2), color = "brown")
```

##
```{r echo=FALSE}
p2  + ggtitle("Wavelet representation with resolution J = 0, 1, 2") + theme(plot.title = element_text(hjust = 0.5))
```

##
```{r fig.show='hide'}
f5 = dwt.fit[[13]]+dwt.fit[[12]] + dwt.fit[[11]] + dwt.fit[[10]]+dwt.fit[[9]] + dwt.fit[[8]]
df = data.frame(x = xi, y = yi, 
  f0 = f0, f1 = f1, f2 = f2, f5 = f5)
p5 = ggplot() + 
  geom_point(data  = df , 
    aes(x = xi, y = yi)) +
  geom_line(data = df, 
    aes(x = xi, y = f0), color = "blue")+
  geom_line(data = df, 
    aes(x = xi, y = f1), color = "red")+
  geom_line(data = df, 
    aes(x = xi, y = f2), color = "brown")+
  geom_line(data = df, 
    aes(x = xi, y = f5), color = "darkgreen")
```



##
```{r echo=FALSE}
p5 + ggtitle("Wavelet representation with increasing resolution J = 0, 1, 2, 5") + theme(plot.title = element_text(hjust = 0.5))

```


##
- \rc What if we choose $J$ is less than 12 for this example?\bc
- Set $J = 3$
- $j0 > 0$, for example, when $J = 3$, $j0 = 9$. 
```{r}
dwt.fit.J3 = mra(yi, method="dwt", wf="haar", J=3)
```

##
```{r fig.show='hide'}
length(dwt.fit.J3)
f9 = dwt.fit.J3[[4]] # f0
f10 = dwt.fit.J3[[4]] + dwt.fit.J3[[3]] # f1
f11 = dwt.fit.J3[[4]] + dwt.fit.J3[[3]] + dwt.fit.J3[[2]]# f2
df = data.frame(x = xi, y = yi, 
  f0 = f9, f1 = f10, f2 = f11)
p.J3 = ggplot() + 
  geom_point(data  = df , 
    aes(x = xi, y = yi)) +
  geom_line(data = df, 
    aes(x = xi, y = f0), color = "blue")+
  geom_line(data = df, 
    aes(x = xi, y = f1), color = "red")+
  geom_line(data = df, 
    aes(x = xi, y = f2), color = "brown")
```

##
```{r echo=FALSE}
p.J3
```

##

- `dwt` determines the wavelet coefficients at each resolution level.
    - `n.levels` - resolution levels to determine.
- Read Page 637 for more detail.
```{r}
y.dwt <- dwt(yi, wf="haar", n.levels=12)
```

## 
- The resulting R list of coefficients may be used to reconstruct the original vector of sampled data $y$.
```{r}
reconstruct.y = idwt(y.dwt)
plot(xi, reconstruct.y)
```

# Wavelet Thresholding 

##
- We saw how a function $f$ may be represented with a wavelet basis.
    - DWT, a sample of length $n$ from $f$ may be decomposed into $n$ wavelet coefficients making up a single smooth approximation and up to $J = \text{log}_{2}\left(n\right).$ detail resolution levels.
- Sparsity - the ability of wavelets to represent a function by concentrating or compressing the information about $f$ into a few large magnitude coefficients and many small magnitude coefficients.
- Compression (thresholding) is applied to the wavelet coefficients of a sampled function $f$ prior to its reconstruction.
- Thresholding - provides a significant level of data reduction for the problem.

## Sparsity of the Wavelet Representation
```{r echo=FALSE, fig.width=1, fig.height=1}
library(png)
library(grid)
img <- readPNG("sunspot.png")
grid.raster(img)
```

- **HWC** Example 13.3
- Monthly sunspot numbers from 1749 to 1983. 
- Sunspots are temporary phenomena on the photosphere of the
sun that appear visibly as dark spots compared to surrounding
regions.


## 
- Sunspots correspond to concentrations of magnetic field flux that
inhibit convection and result in reduced surface temperature
compared to the surrounding photosphere.
- The original data has length 2820, but only the first 2048 are
used here to make it a dyadic number.
- So the filtered data is monthly sunspot data from January 1749 through July 1919.

```{r}
library(datasets)
data(sunspots)
```

##
```{r fig.width=15, fig.height= 10}
plot.ts(sunspots[1:2048], 
  ylab = "Number of sunspots", 
  xlab = "Months") 
```


##
- The DWT is applied to this data resulting in 2048 coefficients. 
```{r}
dwt.sunspot = dwt(sunspots[1:2048], n.levels = 4, wf = "la8")
```

- These coefficients are sorted in magnitude and the smallest 50% (1024) are set to 0.
    - Reconstruction nearly indistinguishable from the original data.
```{r}
dwt.sunspot.coeff = unlist(dwt.sunspot)
dwt.sunspot.coeff = sort(dwt.sunspot.coeff, 
  decreasing = T)
val = as.numeric(quantile(dwt.sunspot.coeff, 
  p =.5))
manual.thresholding = manual.thresh(dwt.sunspot, 
  value = val)
```

##
- The inverse DWT is applied to this compressed (50\% thresholding) set of coefficients, resulting in the reconstruction.

```{r fig.show='hide'}
y.idwt.manual.thresholding = idwt(manual.thresholding)
plot(y.idwt.manual.thresholding, 
  ylab = "Number of sunspots", 
  xlab = "Months",
  main = "50% Thresholding")
```

##
```{r echo=FALSE, fig.width=15, fig.height= 10}
plot(y.idwt.manual.thresholding, 
  ylab = "Number of sunspots", 
  xlab = "Months",
  main = "50% Thresholding")
```

##
- Set smallest 95\% of the coefficients to 0 prior to reconstruction.
    - Reconstruction with the basic shape of the original data, but with the very localized variability mostly removed.

##
```{r echo=FALSE, fig.width=15, fig.height= 10}
val = as.numeric(quantile(dwt.sunspot.coeff, p =.95))
manual.thresholding = manual.thresh(dwt.sunspot, value = val )
y.idwt.manual.thresholding = idwt(manual.thresholding)
plot(y.idwt.manual.thresholding, 
  ylab = "Number of sunspots", 
  xlab = "Months",
  main = "95% Thresholding")
```



## Thresholding
- A drawback to compression - need to specify the amount of reduction.
- Thresholding specifies a data-driven compression.
- Many methods of thresholding are based on assuming that the errors are normally distributed.

## Thresholding
- Let $\theta$ is a coefficient estimated with the DWT and $\lambda$ is a specified threshold value.
- Hard thresholding
    - sets a coefficient to 0 if it has small magnitude and leaves the coefficient unmodified otherwise.
- Soft thresholding
    - threshold sets small coefficients to 0 and shrinks the larger ones by $\lambda$ toward 0.
- DWT operation may be represented as a matrix operator $W$ $$\tilde{\theta} = Wf + W \epsilon.$$
    - $\theta =  W f$ represents the wavelet coefficients of the unobserved sampled function $f$.
    - $\tilde{\epsilon} = W \epsilon$ represents the coefficients of the errors.
    
## Thresholding 

```{r echo=FALSE}
library(png)
library(grid)
img <- readPNG("hard_soft_thresholding.png")
grid.raster(img)
```
    
## Thresholding - VisuShrink (Donoho and Johnstone (1994))

- Applying a single threshold $\lambda$ [@donoho1994].

```{r fig.show='hide'}
y = sunspots[1:2048]
y.dwt = dwt(sunspots[1:2048])
y.visuShrink = universal.thresh(y.dwt, hard = TRUE)
y.idwt.visuShrink = idwt(y.visuShrink)
plot(y.idwt.visuShrink,  
  ylab = "Number of sunspots", 
  xlab = "Months",
  main = "VisuShrink-Hard Thresholding")
```

##
```{r echo=FALSE, fig.width=15, fig.height= 10}
plot(y.idwt.visuShrink, 
  ylab = "Number of sunspots", 
  xlab = "Months",
  main = "VisuShrink-Hard Thresholding")
```

##
```{r fig.show='hide'}
y.visuShrink.soft = universal.thresh(y.dwt, hard = FALSE)
y.idwt.visuShrink.soft = idwt(y.visuShrink.soft)
plot(y.idwt.visuShrink.soft, 
  ylab = "Number of sunspots", 
  xlab = "Months",
  main = "VisuShrink-Soft Thresholding")
```

##
```{r echo=FALSE, fig.width=15, fig.height= 10}
plot(y.idwt.visuShrink.soft, 
  ylab = "Number of sunspots", 
  xlab = "Months",
  main = "VisuShrink-Soft Thresholding")
```

## Thresholding - SureShrink (Donoho and Johnstone (1995))

- Uses a different threshold at each resolution level of the wavelet decomposition of $f$ [@donoho1995].
- SureShrink is actually a hybrid threshold method
    - certain resolution levels can be too sparse.
    - revert `SureShrink` to using the universal threshold of `VisuShrink` at the resolution level in question.
```{r fig.show='hide'}
y.sureshrink = hybrid.thresh(y.dwt, max.level = 4)
y.sureshrink.idwt = idwt(y.sureshrink)
plot(y.sureshrink.idwt, 
  ylab = "Number of sunspots", 
  xlab = "Months",
  main = "SureShrink-Soft Thresholding")
```

##
```{r echo=FALSE, fig.width=15, fig.height= 10}
plot(y.sureshrink.idwt, 
  ylab = "Number of sunspots", 
  xlab = "Months",
  main = "SureShrink-Soft Thresholding")
```

<!-- ## -->
<!-- ```{r fig.show='hide'} -->
<!-- y.sureshrink.hard = sure.thresh(y.dwt,  -->
<!--   max.level = 4, hard = TRUE) -->
<!-- y.sureshrink.hard.idwt = idwt(y.sureshrink.hard) -->
<!-- plot(y.sureshrink.hard.idwt,  -->
<!--   ylab = "Number of sunspots",  -->
<!--   xlab = "Months", -->
<!--   main = "SureShrink-Hard Thresholding") -->
<!-- ``` -->

<!-- ## -->
<!-- ```{r echo=FALSE, fig.width=15, fig.height= 10} -->
<!-- plot(y.sureshrink.hard.idwt,  -->
<!--   ylab = "Number of sunspots",  -->
<!--   xlab = "Months", -->
<!--   main = "SureShrink-Hard Thresholding") -->
<!-- ``` -->

## Other use of wavelets
- Nonparametric density estimation (Vidakovic (1999)).
- Use for understanding the properties of time series and random processes.

## Notes
- Can do thresholding without strong distributional assumptions on the errors using cross-validation [@nason1996].
- Practical, simultaneous confidence bands for wavelet estimators
are not available (Wasserman 2006). 
- Standard wavelet basis functions are not invariant to
translation and rotations.
    - Recent work by [@mallat2012] and [@bruna2013]
extend wavelets to handle these kind of invariances.
    - Promising new direction for the theory of
convolutional neural network.

##  References for this lecture


**HWC** Chapter 13 (Wavelets)

**W** Chapter 9 

<!-- Homework 6 Page 644 Problem 6 (a). -->
