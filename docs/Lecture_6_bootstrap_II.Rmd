---
title: "Lecture 6: Bootstrap II"
shorttitle: "STATS 205 Lecture 6"
author: "Pratheepa Jeganathan"
date: "04/15/2019"
output: 
  beamer_presentation:
    colortheme: "seahorse"
    slide_level: 2
    includes:
      in_header: header.tex
bibliography: nonparamterics.bib

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width = 3,fig.height = 3,message=FALSE, warning=FALSE)
```

##  Recall

- Jackknife for bias and standard error of an estimator.
- Bootstrap samples, bootstrap replicates.
- Bootstrap standard error of an estimator.
- Bootstrap percentile confidence interval.
- Hypothesis testing with the bootstrap (one-sample problem.)
- Assessing the error in bootstrap estimates.
- Example: inference on ratio of heart attack rates in aspirin-intake group to placebo group.

##  Complete Enumeration

- Number of bootstrap samples $\vX^{*} = \lbrace x^{*}_{1}, \cdots,x^{*}_{n} \rbrace$ is $n^{n}$.
- Not all these sets are different.
    - For example with $n =3$, $\lbrace x_{1}, x_{1}, x_{3} \rbrace$ is same as $\lbrace x_{3}, x_{1}, x_{1} \rbrace$.
- Group together bootstrap samples that generate the same subset.
- Characterize each bootstrap sample by its weight vector $\left(k_{1}, \cdots, k_{n} \right)$, where $k_{i}$ is the number of times $x_i$ appears in the bootstrap sample. Thus, $k_{1}+  \cdots k_{n} = n$.
- Let the space of compositions of $n$ into at most $n$ parts be 
$\mathcal{C}_{n}=\{{\vk}=\left(k_1,\cdots,k_n\right),\; k_1+\cdots +k_n=n,\; k_i\geq 0,\;
k_i\;\hbox{integer}\}.$

## Complete Enumeration
- Size of the space $\mathcal{C}_{n}$ is $|\mathcal{C}_{n}| = {2n-1 \choose n-1}$.
    - Each component in the vector $\left(k_1,\cdots,k_n\right)$ is considered to be a box. There are $n$ boxes to contain $n$ balls in all.
    -  We want to count the number of ways of separating the $n$ balls into the $n$ boxes.
    - Put $(n-1)$ separators of $|$ to make boxes and $n$ balls.
        - For example, if $n=3$ and $\vX = \lbrace x_{1}, x_{2}, x_{3}\rbrace$.
        - o|o|o corresponds to $\vX^{*1} = \lbrace x_{1}, x_{2}, x_{3}\rbrace$.
        - oo||o corresponds to $\vX^{*2} = \lbrace x_{1}, x_{1}, x_{3}\rbrace$.
    - $2n-1$ positions from which to choose $n-1$ bars positions.
- Each bootstrap sample corresponds to sampling weight $\vk = \left(k_1,\cdots,k_n\right) \sim \text{Multinomial}\left(n, \vp\right),$ where $\vp = \left(p_{1}, \cdots, p_{n}\right)$ and $p_{i} = \dfrac{1}{n} \hspace{.1in} \forall i$.


## The exhaustive bootstrap

- The exhaustive bootstrap distribution of a statistic $T\left(\vX\right)$
    - compute each of the ${2n-1\choose n-1}$ statistics and 
    - associate a weight $\vk \sim \text{Multinomial}\left(n, \vp\right)$ with it.
- The shift from space of possible resamples with replacement to $\mathcal{C}_{n}$ gives substantial savings.
    - For example with $n=10$, the number of enumerations reduce from $10^{10} \approx 1 \times 10 ^{10}$ to $92378$.

##  Compare bootstrap method using Monte Carlo simulations and exhaustive bootstrap
- **W** Chapter 3.8: The data are LSAT scores (for entrance to law school) and GPA. This data were used to illustrate the bootstrap by Bradly Efron, the inventor of the bootstrap. 

```{r}
library(bootstrap);
data(law)
t(law)
```

## Example (scatterplot)

```{r fig.show='hide'}
library(ggplot2)
ggplot(data = law, aes(x= LSAT, y= GPA))
```

## Example (scatterplot)

```{r echo=FALSE}

ggplot(data= law, aes(x= LSAT, y= GPA)) +
  geom_point() +
  geom_text(aes(label=rownames(law)),hjust=0, vjust=0)
```

## Example (Plug-in estimate of the correlation coefficient)
```{r}
theta.hat = cor(law$LSAT, law$GPA)
theta.hat
```


## Example (bootstrap replicates)
```{r}
draw.bootstrap.samples = function(df){
  n = dim(df)[1]
  ind = sample(n, replace = TRUE)
  cor.bootstrap.replicate = cor(df[ind, "LSAT"], df[ind, "GPA"])
  return(cor.bootstrap.replicate)
}
R = 10000
theta.hat.star = replicate(R, draw.bootstrap.samples(law))
```

## Example (bootstrap approximation for the sampling distribution of plug-in estimator)
```{r fig.show='hide'}
theta.hat.star.df = data.frame(theta.hat.star = theta.hat.star)
ggplot(theta.hat.star.df) +
  geom_density(aes(x = theta.hat.star, y = ..scaled..), 
    fill = "lightblue") + 
  geom_hline(yintercept=0, colour="white", size=1) + 
  theme_bw() + 
  ylab("density") +
  xlab(bquote(hat(theta))) +
  geom_vline(xintercept = theta.hat, col = "red")+
  scale_y_continuous(expand = c(0,0)) 
```

## Example (bootstrap approximation for the sampling distribution of plug-in estimator)
```{r echo=FALSE}
theta.hat.star.df = data.frame(theta.hat.star = theta.hat.star)
ggplot(theta.hat.star.df) +
  geom_density(aes(x = theta.hat.star, y = ..scaled..), 
    fill = "lightblue") + 
  geom_hline(yintercept=0, colour="white", size=1) + 
  theme_bw() + 
  ylab("density") +
  xlab(bquote(hat(theta))) +
  geom_vline(xintercept = theta.hat, col = "red") +
  scale_y_continuous(expand = c(0,0)) 
```

## Example (standard error using bootstrap)
```{r}
sd(theta.hat.star)
```

## Example (the exhaustive bootstrap distribution of the plug-in estimate)
- Create matrix of all ${2n-1 \choose n-1}$ enumerations.
```{r}
library(partitions)
n = 15
allCompositions = compositions(n, n)
```

## Example (the exhaustive bootstrap distribution of the plug-in estimate)
```{r}
allCompositions[,1:10]
```

## Example (the exhaustive bootstrap distribution of the plug-in estimate)
- Check number of compositions
```{r }
dim(allCompositions)[2] == choose((2*n-1), (n-1))
```

## Example (the exhaustive bootstrap distribution of the plug-in estimate)

- Compute ${2n-1 \choose n-1}$ bootstrap replicates.

```{r eval=FALSE}
library(parallel)
nCompositions = dim(allCompositions)[2]
t.start = proc.time()
enumData = mclapply(1:nCompositions, function(i) {
    ind = allCompositions[,i]
    law.list = lapply(1:n,function(j) matrix(rep(law[j,], ind[j]),ncol = 2 ,byrow = TRUE))
    newLaw = do.call(rbind, law.list)
    c(cor(unlist(newLaw[,1]),unlist(newLaw[,2])),dmultinom(ind,prob = rep(1,n)))
    }, mc.cores = 4)
proc.time() - t.start

enumData = t(simplify2array(enumData))
colnames(enumData) = c("ex.theta.hat.star","weight")
save(enumData,file = "enumData.Rdata") 
```


## Example (the exhaustive bootstrap distribution of the plug-in estimate)

```{r eval=FALSE}
load("enumData.Rdata")
ex.theta.hat.star.df = 
  data.frame(ex.theta.hat.star = 
      enumData$ex.theta.hat.star)
ggplot(ex.theta.hat.star.df) +
  geom_density(aes(x = ex.theta.hat.star, y = ..scaled..), 
    fill = "lightblue") + 
  geom_hline(yintercept=0, colour="white", size=1) + 
  theme_bw() + 
  ylab("density") +
  xlab(bquote(hat(theta))) +
  geom_vline(xintercept = theta.hat, col = "red") + 
  ggtitle("The exhaustive bootstrap distribution of the plug-in estimate")

```


```{r include=FALSE, eval=FALSE}
ind = allCompositions[,1]
law.star = lapply(1:n, function(j) matrix(rep(law[j,],ind[j]), ncol = 2 ,byrow = TRUE))
law.star = do.call(rbind, law.star) %>% data.frame
weight = dmultinom(ind,prob = rep(1,n))
ex.theta.hat.star = cor(law.star[,1], law.star[,2])

```

## Gray Codes to speed up the enumeration
- We can speedup enumeration by changing only one coordinates at the time using Gray codes.
- Suggested reading: 
    - **Re:DH1994**: Diaconis and Holmes (1994). [Gray Codes for Randomization Procedures.](https://link.springer.com/article/10.1007/BF00156752)

<!-- ## Exploring the tails of a bootstrap distribution -->


##  References for this lecture

**W** Chapter 3.8 (The bootstrap and the jackknife).

**Li:C2016:** [Seiler (2016). Lecture Notes on Nonparametric Statistics - bootstrap example.](http://christofseiler.github.io/stats205/Lecture4/BootstrapExample.html#1)

**Li:H2004:** [Holmes (2004). Lecture Notes on Complete Enumeration.](Li:H2004: Holmes (2004). Lecture Notes on Complete Enumeration)



