---
title: "Lecture 10: Two-sample problem II"
shorttitle: "STATS 205 Lecture 10"
author: "Pratheepa Jeganathan"
date: "04/24/2019"
output: 
  beamer_presentation:
    colortheme: "seahorse"
    slide_level: 2
    includes:
      in_header: header.tex
bibliography: nonparamterics.bib

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width = 3,fig.height = 3, fig.align="center",message=FALSE, warning=FALSE)
```

#  Recall

##
- One sample sign test, Wilcoxon signed rank test, large-sample approximation, median, Hodges-Lehman estimator, distribution-free confidence interval.
- Jackknife for bias and standard error of an estimator.
- Bootstrap samples, bootstrap replicates.
- Bootstrap standard error of an estimator.
- Bootstrap percentile confidence interval.
- Hypothesis testing with the bootstrap (one-sample problem.)
- Assessing the error in bootstrap estimates.
- Example: inference on ratio of heart attack rates in the aspirin-intake group to the placebo group.
- The exhaustive bootstrap distribution
- Discrete data problems (one-sample, two-sample proportion tests, test of homogeneity, test of independence)
- Two-sample problems (location problem - equal variance, unequal variance, exact test or Monte Carlo, large-sample approximation, H-L estimator)

# Two-sample location problem

## Behrens-Fisher problem

- Let $X_{1}, \cdots X_{m}$ and $Y_{1}, \cdots Y_{n}$ be independent random samples from continuous distributions that are symmetric about the population medians $\theta_{X}$ and $\theta_{Y}$, respectively.
- Behrens-Fisher problem: testing $\text{H}_{0}:\theta_{X}= \theta_{Y}$ without assuming equal variances.


## Behrens-Fisher problem (Fligner-Policello)
- $P_{i} = \left[\text{number of sample} \hspace{.1in} \vY \hspace{.1in} \text{observations less than} \hspace{.1in} X_{i} \right], i = 1, \cdots, m$.
- $Q_{i} = \left[\text{number of sample} \hspace{.1in} \vX \hspace{.1in} \text{observations less than} \hspace{.1in} Y_{j} \right], j = 1, \cdots, n$.
- Average $\vX$ sample placement $\bar{P} = \dfrac{\sum_{i=1}^{m}P_{i}}{m}$.
- Average $\vY$ sample placement $\bar{Q} = \dfrac{\sum_{j=1}^{n}Q_{j}}{n}$.
- $V_{1} = \sum_{i=1}^{m} \left(P_{i}-\bar{P}\right)^{2}$.
- $V_{2} = \sum_{j=1}^{n} \left(Q_{j}-\bar{Q}\right)^{2}$.
- The standardized test-statistic $\hat{U} = \dfrac{\sum_{j=1}^{n}Q_{j}- \sum_{i=1}^{m}P_{i}}{2 \sqrt{V_{1} + V_{2}+ \bar{P}\bar{Q}}}$.
- $\hat{U}$ has a symmetric distribution.
- $\hat{U}$ resembles Welch’s t statistic for the normal theory when variances are unequal.

## Behrens-Fisher problem (Fligner-Policello)

- Let $u_{\alpha}$ is the upper $\alpha$ quantile of $\hat{U}$.
- $u_{\alpha}$ can be computed exactly or estimated using Monte Carlo simulation using the R command `cFligPoli`.
- To do exact test (either exact or Monte Carlo)

```{r eval=FALSE}
pFligPoli(x = X, y = Y, 
  method = "Monte Carlo")
```


## Behrens-Fisher problem (large-sample approximation)

- $\hat{U}$ has an asymptotic ($\text{min}(m,n) \to \infty$) $\text{N}\left(0, 1\right)$ distribution.


```{r eval=FALSE}
pFligPoli(x = X, y = Y, 
  method = "Asymptotic")
```


## Example (Behrens-Fihser problem)

- **HWC**: Example 4.5 Plasma Glucose in Geese.
-  Examining whether plasma glucose in lead-poisoned geese are greater than plasma glucose in healthy (normal) Canadian geese.
- Measured plasma glucose in eight healthy and seven lead-poisoned geese.
```{r}
# Table4.7
healthy.geese = c(297,340,325, 227, 
  277, 337, 250, 290)
lead.poisoned.geese = c(293, 291, 289, 
  430, 510, 353, 318)
```

- Let $X$ be plasma glucose in healthy geese and $Y$ be plasma glucose in lead-poisoned geese.

## Example (Behrens-Fihser problem)

- Let $\theta_{X}$ be the location parameter of plasma glucose in healthy geese and $\theta_{Y}$ be the location parameter of plasma glucose in lead-poisoned geese.
- Hypothesis: $\text{H}_{0}: \theta_{X} = \theta_{Y}$ versus $\text{H}_{A}: \theta_{X} < \theta_{Y}$.
- Let's use P-value approach:
- ${8+7 \choose 7} = 6435 < 10,000$ can do exact test.
```{r}
library(NSM3)
pFligPoli(x = healthy.geese, 
  y = lead.poisoned.geese, 
  method = "Exact")
```

## Example (Behrens-Fihser problem)

- Because $\hat{U}$ is symmetric, exact lower-tail probability = P-value = .0808.
- We do not have enough evidence to reject $\text{H}_{0}$ at 5\% significance level.

# Other two-sample problems

## Dispersion test (medians equal - ANSARI–BRADLEY)

- Data: $X_{1}, \cdots X_{m} \sim F\left(.\right)$ and $Y_{1}, \cdots Y_{n} \sim G\left(.\right)$.

- Assumption
    - $X$ and $Y$ are mutually independent.
    - $F\left(.\right)$ and $G\left(.\right)$ are continuous.

- Let $\theta_{X}$, $\theta_{Y}$ be the population medians for the $X$ and $Y$ distributions.
- Let $\eta_{X}$, $\eta_{Y}$ be the scale parameters associated with $X$ and $Y$ distributions.

## Dispersion test (medians equal - ANSARI–BRADLEY)

- Example for the probability distributions with the same general form and equal medians but different scale parameters.
```{r echo=FALSE}
library(tidyr)
library(ggplot2)
df = data.frame(x= rnorm(10000, mean= 0 , sd = 2), 
  y = rnorm(10000, mean = 0, sd = 5 ))
df.long = gather(df, key = "X.Y", value = "value")
df.long$X.Y = factor(df.long$X.Y)
ggplot(df.long, aes(x = value, 
  y = ..scaled.., col = X.Y)) + 
  geom_density() + 
  geom_hline(yintercept=0, 
    colour="white", size=1) +
  theme_bw() + guides(col=FALSE) + 
  ylab("") + 
  xlab("")
```

## Dispersion test (medians equal - ANSARI–BRADLEY)

- Parameter of interest: $\gamma = \dfrac{\eta_{X}}{\eta_{X}}$.
- $\text{H}_{0}: \gamma^{2} = 1$ versus $\text{H}_{A}: \gamma^{2} \neq 1$.
- Other alternative: $\text{H}_{A}: \gamma^{2} > 1$, $\text{H}_{A}: \gamma^{2} < 1$.
- Ansari–Bradley two-sample scale statistic $C$.
    - Assign the score 1 to both the smallest and largest observations in the combined sample.
    - Assign the score 2 to the second smallest and second largest, and continue in the manner.
    - Let $R_{j}$ denote the score assigned in this manner to $Y_{j}, j = 1, \cdots, n$.
    - $C = \sum_{i=1}^{n}R_{j}$.


## Dispersion test (medians equal - ANSARI–BRADLEY)


```{r eval=FALSE}
pAnsBrad(x = X, y = Y, 
  method = "Exact")
```
- If there are ties among the $X$ and/or $Y$ observations, assign each of the observations in a tied group the average of the integer scores that are associated with the tied group.
    - text will not be exact.

## Dispersion test (medians equal - large-sample approximation)

- $\E_{0}\left(C\right) = \dfrac{n(N+2)}{4}$ - expected value of $C$ under $\text{H}_{0}$ and medians are equal.
- $\V_{0}\left(C\right) = \dfrac{mn(N+2)(N-2)}{48(N-1)}$ - variance of $C$ under $\text{H}_{0}$ and medians are equal.
- $Z = \dfrac{C - \E_{0}\left(C\right)}{\sqrt{\V_{0}\left(C\right) }} \sim \text{N}\left(0, 1\right)$.

```{r eval=FALSE}
pAnsBrad(x = X, y = Y, 
  method = "Asymptotic")
```

## Example (Dispersion test)

- **HWC** Example 5.1 (Serum Iron Determination)
- From the point of view of procedural technique, the Jung–Parekh method competes favorably with the Ramsay method for serum iron determination.
- Test whether loss of accuracy when the Jung–Parekh procedure is used instead of the Ramsay procedure.
    - The alternative of interest in this example is greater dispersion or variation for the Jung–Parekh method of serum iron determination than for the method of Ramsay.
    - $\text{H}_{A}: \gamma^{2} > 1$.
```{r}
serum = list(ramsay = c(111, 107, 100, 99, 102, 
  106, 109, 108, 104, 99, 101, 96, 97, 102, 107,
  113, 116, 113, 110, 98),
jung.parekh = c(107, 108, 106, 98, 105, 103, 
  110, 105, 104, 100, 96, 108, 103, 104, 114, 
  114,113, 108, 106, 99))
```

##
```{r}
pAnsBrad(serum$ramsay, serum$jung.parekh, 
  method = "Asymptotic")
```

- P-value for the upper-tail test is .9093.
- There is absolutely no evidence in the sample data to indicate any loss of accuracy with the Jung–Parekh method.

## Dispersion problem (medians are unequal - MILLER(JACKKNIFE))

- Read **HWC 5.2**
- Ties: no adjustments are necessary - the jackknife procedures are well defined when ties within or between the $X$'s and $Y$’s occur.
- Compute Miller Jackknife Q statistic.

```{r eval=FALSE}
Q = MillerJack(x = X, y = Y)
```

- One-sided P-value (when $n,m$ small.)
```{r eval=FALSE}
1 - pt(Q)
```

- One-sided P-value (when $n,m$ large.)
```{r eval=FALSE}
1 - pnorm(Q)
```


## General distribution test (KOLMOGOROV SMIRNOV)

- $X_{1}, \cdots X_{m} \sim F\left(.\right)$ and $Y_{1}, \cdots Y_{n} \sim G\left(.\right)$.
- $\text{H}_{0}: F\left(t\right) = G\left(.\right) \hspace{.1in} \forall t$ versus $\text{H}_{A}: F\left(t\right) \neq G\left(.\right) \hspace{.1in} \text{for at least one} \hspace{.1in} t$.
- Define empirical distribution functions for the $X$ and $Y$ samples.
    - $F_{m}\left(t\right) = \dfrac{\#\lbrace X's \leq t\rbrace}{m}$.
    - $G_{n}\left(t\right) = \dfrac{\#\lbrace Y's \leq t\rbrace}{n}$.
- Define $d$ is the greatest common divisor of $m$ and $n$.
```{r}
library(FRACTION)
gcd(4,3)
```



## General distribution test (KOLMOGOROV SMIRNOV)

- Kolmogorov–Smirnov general alternative (two-sided) statistic $J$
    - $J = \dfrac{mn}{d} \maxu{i = 1, \cdots, N} \lbrace |F_{m}\left(Z_{(i)}\right)  - G_{n}\left(Z_{(i)}\right)|\rbrace$, where $Z_{(1)}, \cdots, Z_{(N)}$ are ordered values for the combined sample.
- Reject $\text{H}_{0}$ if $J \geq j_{\alpha}$, where $j_{\alpha}$ is the upper $\alpha$ percentile of $J$ under $\text{H}_{0}$.
    - Due to discreteness, $j_{\alpha}$ is not defined for all $\alpha$ values.
- Ties: no adjustments are necessary because empirical distributions $F_{m}(t)$ and $G_{n}(t)$ are well defined.

```{r eval=FALSE}
pKolSmirn(x= X, y = Y, 
  method = "Exact")
```

## General distribution test (large-sample approximation)
- Smirnov (1939)
- The large-sample $\left(\text{min}\left(m,n\right) \to \infty\right)$ approximation is based on the asymptotic distribution of $J$.
- $J^{*} = \dfrac{d}{\sqrt{mnN}}J \sim Q\left(\cdot\right)$.
- For the alternative $\text{H}_{A}: F\left(t\right) \neq G\left(.\right) \hspace{.1in} \text{for at least one} \hspace{.1in} t$, reject $\text{H}_{0}$ if $J^{*} \geq q_{\alpha}$.

```{r eval=FALSE}
pKolSmirn(x = X, y = Y, 
  method = "Asymptotic")
```


## Example (General distribution test)

- **HWC** Example 5.4 (Effect of Feedback on Salivation Rate.)
- Interest: The effect of enabling a subject to hear himself salivate while trying to increase or decrease his salivary rate.
- Experiment: Two groups of subjects were told to attempt to increase their salivary rates upon observing a light to the left and decrease their salivary rates upon observing a light to the right.
- Data: collected amount of saliva on feedback and no-feedback groups.
```{r}
Table5.7 = list(feedback = c(-0.15, 8.6, 5, 3.71, 
  4.29, 7.74, 2.48, 3.25, -1.15, 8.38), 
  no.feedback = c(2.55, 12.07, 0.46, 0.35, 2.69,
    -0.94, 1.73, 0.73, -0.35, -0.37))
```

## Example (General distribution test)
- Exact test
```{r}
pKolSmirn(x = Table5.7$feedback, 
  y = Table5.7$no.feedback, method = "Exact")
```

We conclude that there is some marginal evidence in the samples that feedback might have an effect on salivation rate.

## Example (General distribution test)
- The large-sample test

```{r}
pKolSmirn(x = Table5.7$feedback, 
  y = Table5.7$no.feedback, method = "Asymptotic")
```

Using large-sample test, we reach the same conclusion that there is some marginal evidence in the samples that feedback might have an effect on salivation rate.

## Summary
- Testing procedure
    - Two-sample location problem (variance equal) (Wilcoxon rank sum test).
    - Two-sample location problem (variance unequal) (Fligner-Policello).
    - Two-sample dispersion problem (median equal) (ANSARI–BRADLEY).
    - Two-sample dispersion problem (median unequal) (Jackknife-Miller).
    - General distribution test (KOLMOGOROV SMIRNOV).
- In practice
    - Use boxplot to decide equal median or not.
    - Test for equal distribution.
    - Test for dispersion.
    - Choose appropriate location test.
    - If sample size is larger, choose large sample approximation.
    
##  References for this lecture

**HWC** Chapter 4.4

**HWC** Chapter 5

<!-- Homework problem: Page 149, problem 41. (two sample location when unequal variances) -->

<!-- Homework problem: Page 168, problem 1. (dispersion problem) -->

<!-- Homework problem: Page 198, problem 33 (general distribution test) -->
