---
title: "Lecture 16: Nonparametric regression II"
shorttitle: "STATS 205 Lecture 16"
author: "Pratheepa Jeganathan"
date: "05/08/2019"
output: 
  beamer_presentation:
    colortheme: "seahorse"
    slide_level: 2
    includes:
      in_header: header.tex
bibliography: nonparamterics.bib

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width = 5, fig.height = 5, fig.align="center",message=FALSE, warning=FALSE, out.width = '70%')
```

#  Recall

##
- One sample sign test, Wilcoxon signed rank test, large-sample approximation, median, Hodges-Lehman estimator, distribution-free confidence interval.
- Jackknife for bias and standard error of an estimator.
- Bootstrap samples, bootstrap replicates.
- Bootstrap standard error of an estimator.
- Bootstrap percentile confidence interval.
- Hypothesis testing with the bootstrap (one-sample problem.)
- Assessing the error in bootstrap estimates.
- Example: inference on ratio of heart attack rates in the aspirin-intake group to the placebo group.
- The exhaustive bootstrap distribution.


##
- Discrete data problems (one-sample, two-sample proportion tests, test of homogeneity, test of independence).
- Two-sample problems (location problem - equal variance, unequal variance, exact test or Monte Carlo, large-sample approximation, H-L estimator, dispersion problem, general distribution).
- Permutation tests (permutation test for continuous data, different test statistic, accuracy of permutation tests).
- Permutation tests (discrete data problems, exchangeability.)
- Rank-based correlation analysis (Kendall and Spearman correlation coefficients.)
- Rank-based regression (straight line, multiple linear regression, statistical inference about the unknown parameters, nonparametric procedures - does not depend on the distribution of error term.)

## 
- Smoothing (density estimation, bias-variance trade-off, curse of dimensionality)
- Nonparamteric regression (Local averaging, local regression, kernel smoothing, local polynomial, penalized regression)
 

# Nonparametric regression II

## Introduction
- Cross-Validation
- Variance Estimation
- Confidence Bands
- Bootstrap Confidence Bands

# Choosing smoothing parameter

## Choosing smoothing parameter

- Risk depends on unknown function $r\left(x\right)$. $$R\left(h \right) = \E \left(\dfrac{1}{n}\left(\hat{r}_{n}\left(x_{i}\right) -r\left(x_{i}\right)\right)^{2} \right).$$

1) Training error
    - $\dfrac{1}{n}\sum_{i=1}^{n}\left(Y_{i} - \hat{r}_{n}\left(x_{i}\right)\right)^{2}.$
    - Using data twice.
        - to estimate $r$.
        - to estimate the risk $R$.
    - Function estimate is chosen to make $\dfrac{1}{n}\sum_{i=1}^{n}\left(Y_{i} - \hat{r}_{n}\left(x_{i}\right)\right)^{2}$ small so risk is underestimated.

## Choosing smoothing parameter

2) Leave-one-out cross-validation score $$\text{CV} = \hat{R}\left(h \right) = \dfrac{1}{n}\sum_{i=1}^{n}\left(Y_{i} - \hat{r}_{(-i)}\left(x_{i}\right)\right)^{2}$$
    - $\hat{r}_{(-i)}$ is the estimator obtained by omitting $i$-th pair $\left(x_{i}, Y_{i} \right)$.
    - $\hat{r}_{(-i)}\left(x\right) = \sum_{j=1}^{n}Y_{j}l_{j, (-i)}\left(x \right),$ where 
          \begin{equation}
           l_{j, (-i)}\left(x \right) = 
            \begin{cases}
            0 & \text{if} \hspace{.1in} j = i\\
            \dfrac{l_{j}\left(x\right)}{\sum_{k \neq i}l_{k}\left(x\right) } & \text{if} \hspace{.1in} j \neq i.
            \end{cases}
          \end{equation}
    - Set weight on $x_{i}$ to 0 and renormalize the other weights to sum to one.
    - Do this for different $h$.


## Choosing smoothing parameter
2) Leave-one-out cross-validation

- Intuition: $\E \left(Y_{i} - \hat{r}_{(-i)}\left(x_{i}\right) \right)^{2} \approx \sigma^{2} + \E \left(r\left(x_{i}\right)- \hat{r}_{n}\left(x_{i}\right)\right)^{2}=$  predictive error. $\hat{R}$ score is nearly unbiased estimate of the risk.
- Shortcut formula to compute $\hat{R}$ $$\hat{R}\left(h\right) = \dfrac{1}{n}\sum_{i=1}^{n}\left(\dfrac{Y_{i} - \hat{r}_{n}\left(x_{i}\right)}{1-L_{ii}} \right)^{2},$$ where $L_{ii} = l_{i}\left( x_{i}\right)$ is the $i$-th diagonal element if the smoothing matrix $L$.

## Choosing smoothing parameter

3) Generalized cross-validation $$\text{GCV}\left( h\right) = \dfrac{1}{n}\sum_{i=1}^{n}\left(\dfrac{Y_{i} - \hat{r}_{n}\left(x_{i}\right)}{1-\nu/n} \right)^{2},$$ where $\nu = \text{tr}\left(L\right)$ is the effective degrees of freedom.
    - a formula similar to Colin Mallows $C_{p}$ statistic.

# Variance estimation

## Variance estimation

- We assume $\V \left(\epsilon_{i} \right) = \sigma^{2}.$
    - constant variance 

1) For linear smoother $\vr = \mL \vY$, an unbiased estimate of $\sigma^{2}$ is $$\hat{\sigma}^{2} = \dfrac{\sum_{i=1}^{n}\left(Y_{i} - \hat{r}\left(x_{i}\right) \right)^{2}}{n - 2\nu + \tilde{\nu}},$$ where $\nu = \text{tr}\left(L \right)$ and $\tilde{\nu} = \text{tr}\left(L^{T}L \right) = \sum_{i=1}^{n}\left|\left|l\left(x_{i}\right)\right|\right|^{2}.$

- If $r$ is sufficiently smooth, then $\hat{\sigma}^{2}$ is a consistent estimator of $\sigma^{2}$.

## Variance estimation

2) Alternative formula (Rice 1984).

- Suppose $x_{i}$s are ordered. $$\hat{\sigma}^{2} = \dfrac{1}{2(n-1)}\sum_{i=1}^{n-1}\left(Y_{i+1} - Y_{i}\right)^{2}.$$
- Intuition: an average of the residuals that results from fitting a line to the first and third point of each consecutive triple of design points.

## Variance estimation (Spatially inhomogeneous functions)
- Inhomegenity of variance.
- $\hat{r}_{n}\left(x\right)$ is relatively insensitive to heteroscedastic.
- We need to account for the unconstant variance when making confidence bands.
 
## Example
- Doppler function
```{r fig.show='hide'}
library(ggplot2)
r = function(x){
  sqrt(x*(1-x))*sin(2.1*pi/(x+.05))
}
ep = rnorm(1000)
y = r(seq(1, 1000, by = 1)/1000) + .1 * ep
df = data.frame(x = seq(1, 1000, by = 1)/1000, y = y)
ggplot(df) +
  geom_point(aes(x = x, y = y))
```


## Example
```{r echo=FALSE}
ggplot(df) +
  geom_point(aes(x = x, y = y))
```

## Example

- Doppler function is spatially inhomogeneous (smoothness varies over $x$).
- Estimate by local linear regression

```{r fig.show='hide'}
library(np)
doppler.npreg <- npreg(bws=.005, 
  txdat=df$x,
     tydat=df$y, 
  ckertype="epanechnikov")

doppler.npreg.fit = data.frame(x = df$x, 
  y = df$y, 
  kernel.fit = fitted(doppler.npreg))

p = ggplot(doppler.npreg.fit) + 
  geom_point(aes(x = x, y = y)) + 
  geom_line(aes(x = x, y= kernel.fit), color = "red")  
```

## Example

```{r echo=FALSE}
p 
```

## Example

- Doppler function fit using local linear regression.
    - Effective degrees of freedom 166.
    - Fitted function is very wiggly.
    - If we smooth more, right-hand side of the fit would look better at the cost of missing structure near $x = 0$.
    
- Wavelets 

## Variance estimation
- Estimate $r\left(x\right)$ with any nonparamteric method to get $\hat{r}_{n}\left(x\right)$.
- Compute the squared residuals $Z_{i} = \left(Y_{i}- \hat{r}_{n}\left(x_{i}\right)\right)^{2}$.
- Regress $Z_{i}$ on $x_{i}$ to get an estimate $\hat{q}\left(x\right).$
- $\hat{\sigma}\left(x \right) = \hat{q}\left(x\right)$.

# Confidence Bands

## Confidence Bands

- Can we get confidence bands for $r\left(x\right)$?
- Let mean and standard deviation of $\hat{r}_{n}\left(x\right)$ is $\bar{r}_{n}\left(x\right)$ and $\hat{s}_{n}\left(x\right)$, respectively.
- Bias Problem: 
\begin{equation}
\label{biasProblem}
  \begin{split}
  \dfrac{\hat{r}_{n}\left(x\right)-r\left(x\right)}{\hat{s}_{n}\left(x\right)} = &\dfrac{\hat{r}_{n}\left(x\right)-\bar{r}_{n}\left(x\right)}{\hat{s}_{n}\left(x\right)} + \dfrac{\bar{r}_{n}\left(x\right)-r\left(x\right)}{\hat{s}_{n}\left(x\right)}\\
  = & Z_{n}\left(x\right) + \dfrac{\text{bias}\left(\hat{r}_{n}\left(x\right) \right)}{\sqrt{\text{variance}\left(\hat{r}_{n}\left(x\right) \right)}}.
  \end{split}
\end{equation}
- Typically $Z_{n}\left(x\right) = \dfrac{\hat{r}_{n}\left(x\right)-\bar{r}_{n}\left(x\right)}{\hat{s}_{n}\left(x\right)}$ follows a standard normal and used to derive confidence bands
- In nonparametric regression, the second term in \eqref{biasProblem} does not vanish.
    - Optimal smoothing balance between bias and the standard deviation. 


## Confidence Bands

- Confidence bands for $\bar{r}_{n}\left( x\right)$ is $$\hat{r}_{n}\left(x\right) \pm c \times \text{se}\left(x\right),$$ where $c > 0$ some constant.
- $\bar{r}_{n}\left( x\right) = \E \left(\hat{r}_{n}\left(x\right) \right)$.
    - We don't get a confidence band for $r\left(x\right)$.
- $c$ is computed from the distribution of the maximum of a Gaussian process. Choose $c$ by solving $$2\left(1-\Phi\left(c \right)\right) + \dfrac{\kappa_{0}}{\pi}\text{e}^{-c^{2}/2}  = \alpha,$$ where $\kappa_{0} = \int_{a}^{b} \left|\left| T^{'}\left(x\right)\right|\right|$ and $T_{i}\left(x\right) = \dfrac{l_{i}\left(x\right)}{\left|\left| l_{i}\left(x\right)\right|\right|}.$

## Confidence Bands
- To get simultaneous confidence band, compute $c$ such that $$2 \left(1-\phi \left( c \right) \right)+\dfrac{\kappa_{0}}{\pi}\text{e}^{c^{2}/2}= \alpha.$$
- The variance of $\hat{r}_{n}\left( x\right)$ is $$\V \left( \hat{r}_{n}\left( x\right)\right)=\sum_{i=1}^{n}\sigma^{2}\left(x_{i}\right)l_{i}^{2}\left(x_{i}\right).$$
- The approximate confidence band is $$\mathbb{I}\left(x\right) = \hat{r}_{n}\left(x\right) \pm c \sqrt{\sum_{i=1}^{n}\hat{\sigma}^{2}\left(x_{i}\right)l_{i}^{2}\left(x_{i}\right)}.$$


## Bootstrap Confidence Bands
- Reference: \blc [link here](https://www.stat.cmu.edu/~cshalizi/402/lectures/08-bootstrap/lecture-08.pdf#page20). \bc

1) Resample rows:
    - Resample $\left(x, y \right)$ pair.
    
2) Resample residuals:
    - Hold the $x$ fixed, but make $T$ equal to $\hat{r}\left(x \right)$ plus a randomly re-sampled $\epsilon_{i}$.
    - Errors need to be iid.
  
## Bootstrap Confidence Bands (Example)
- Resample rows
```{r}
library(NSM3)
library(dplyr)
data("ethanol")
ethanol.df = select(ethanol, 
  c(E, NOx))

resample.data = function(df) {
sample.rows = sample(1:nrow(df), 
  replace = TRUE)
return(df[sample.rows,])
}
```

## Bootstrap Confidence Bands (Example)

```{r}
# use kernel smoothing
library(np)
npr.nox.on.E = function(df.star) {
  bw = npregbw(NOx ~ E, 
    data = df.star)
  fit =  npreg(bw)
  return(fit)
}
```

## Bootstrap Confidence Bands (Example)

```{r}
# Use uniform grid points to predict the values.
evaluation.points = seq((min(ethanol.df$E) -.1),
  (max(ethanol.df$E)+.1), by =.01)

eval.npr = function(npr) {
   return(predict(npr, 
     exdat = evaluation.points))
}

ethanol.npr = npr.nox.on.E(ethanol.df)
obs.curve = eval.npr(ethanol.npr)
```

## Bootstrap Confidence Bands (Example)

```{r}
npr.cis = function(B,alpha, df, obs.curve) {
  tboot= replicate(B, 
    eval.npr(npr.nox.on.E(resample.data(df))))
  low.quantiles = apply(tboot, 1, 
    quantile, 
    probs = alpha/2)
  high.quantiles = apply(tboot, 1, 
    quantile, 
    probs = (1-alpha/2))
  low.cis = 2*obs.curve - high.quantiles
  high.cis = 2*obs.curve - low.quantiles
  cis <- rbind(low.cis, high.cis)
  return(list(cis=cis, tboot= t(tboot)))
}
```

## Bootstrap Confidence Bands (Example)

```{r}
ethanol.npr.cis = npr.cis(B = 100, 
  alpha = 0.05, 
  df = ethanol.df, 
  obs.curve = obs.curve)
```

## Bootstrap Confidence Bands (Example)

```{r fig.show='hide'}
df.plot.ci = data.frame(x = evaluation.points, 
  obs.curve = obs.curve,
  low.cis = ethanol.npr.cis$cis[1,], 
  upper.cis = ethanol.npr.cis$cis[2,])

p = ggplot() + 
  geom_point(data = ethanol.df, 
    aes(x = E, y = NOx)
    ) +
  geom_line(data = df.plot.ci, 
    aes(x = evaluation.points, y = low.cis), 
    color = "red", linetype = "dashed", 
    size = 1) 

```

## ## Bootstrap Confidence Bands (Example)
```{r}
p = p +
  geom_line(data = df.plot.ci, 
    aes(x = evaluation.points, y = upper.cis), 
    color = "red", linetype = "dashed", 
    size = 1) +
  geom_line(data = df.plot.ci, 
    aes(x = evaluation.points, y = obs.curve), 
    color = "blue", 
    size = 1)
```


##

```{r echo=FALSE}
p
```

##
- Notes
    - Confidence bands get wider where
there is less data.
    - If variance is not constant, use resampling residuals with heteroskedasticity method describe in the following \blc [link 4.4](https://www.stat.cmu.edu/~cshalizi/402/lectures/08-bootstrap/lecture-08.pdf#page20). \bc

##  References for this lecture

**W** Chapter 5 

Reference for bootstrap confidence bands: \blc [link here](https://www.stat.cmu.edu/~cshalizi/402/lectures/08-bootstrap/lecture-08.pdf#page20). \bc

<!-- Homework 6, W 2006: Page 121, problem 3.  -->
<!-- Get the data on fragments of glass collected in forensic work from the book website. Let $Y$ be refractive index and let $x$ be aluminium content (the fourth variable). Perform a nonparametric regression to fit the model $Y = r\left(x\right)+\epsilon$. Use the following estimators: (i) regressogram, (ii) kernel, (iii) local linear kernel regression, (iv) spline. In each case, use cross-validation to choose the amount of smoothing. Estimate the variance. Construct 95 percent confidence bands for your estimates. -->
