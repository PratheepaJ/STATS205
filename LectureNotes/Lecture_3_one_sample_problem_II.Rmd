---
title: "Lecture 3: One-sample problem II"
shorttitle: "STATS 205 Lecture 3"
author: "Pratheepa Jeganathan"
date: "04/08/2019"
output: 
  beamer_presentation:
    colortheme: "seahorse"
    slide_level: 2
    includes:
      in_header: header.tex
bibliography: nonparamterics.bib

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width = 3,fig.height = 3,message=FALSE, warning=FALSE)
```


# Sign test (Fisher) - paired replicates data/one-sample data

## Sign test 

- $Z_{1}, \cdots\ Z_{n}$ random sample from a continuous population that has a common median $\theta$.
    - If $Z_{i} \sim F_{i}$, $F_{i}\left(\theta\right) = F_{i}\left(Z_{i} \leq \theta \right) = F_{i}\left(Z_{i} > \theta \right) = 1- F_{i}\left(\theta\right)$.
- Hypothesis testing: 
    - $\text{H}_{0}: \theta = 0$ versus $\text{H}_{A}: \theta \neq 0$.

## Sign test (Cont.)

- Sign test statistic: $B = \sum_{i=1}^{n}\psi_{i}$.
- Motivation:
    - When $\theta$ is larger than 0, there will be larger number of positive $Z_{i}$s $->$ big $B$ value $->$ reject $\text{H}_{0}$ in favor of $\theta > 0$.
- Under $\text{H}_{0}$, $B \sim \left(n, 1/2 \right)$
- Significance level $\alpha$: probability of rejecting $H_{0}$ when it is true.
- Note
    - choices of $\alpha$ are limited to possible values of the $B \sim \left(n, 1/2 \right)$ cdf.
    - compare the distribution of $B$ under $\text{H}_{0}$ and the observed test statistic value.


## Sign test (Cont.)

- Rejection regions
    - $\text{H}_{A}: \theta > 0$, Reject $\text{H}_{0}$ if $B \geq b_{\alpha; n, 1/2}$.
    - $\text{H}_{A}: \theta < 0$, Reject $\text{H}_{0}$ if $B \leq n-b_{\alpha; n, 1/2}$.
    - $\text{H}_{A}: \theta \neq 0$, Reject $\text{H}_{0}$ if $B \geq b_{\alpha/2; n, 1/2}$ or $B \leq n-b_{\alpha/2; n, 1/2}$.

## Large-Sample Approximation (Sign test)

- $B^{*} = \dfrac{B- \E_{0}\left(B\right)}{{\V_{0}\left(B\right)}^{1/2}} \sim \text{N}\left(0,1\right) \hspace{.2in} \text{as} \hspace{.2in} n 	\to \infty \hspace{.2in} , \text{where}$
- $\E_{0}\left(B\right) = \dfrac{n}{2}$ and $\V_{0}\left(B\right)= \dfrac{n}{4}$

- Rejection regions
    - $\text{H}_{A}: \theta > 0$, Reject $\text{H}_{0}$ if $B^{*} \geq z_{\alpha}$.
    - $\text{H}_{A}: \theta < 0$, Reject $\text{H}_{0}$ if $B^{*} \leq -z_{\alpha}$.
    - $\text{H}_{A}: \theta \neq 0$, Reject $\text{H}_{0}$ if $B^{*} \geq z_{\alpha/2}$ or $B \leq -z_{\alpha/2}$.
    

## Ties  (Sign test)

- Discard zero $Z$ values and redefine $n$.
- If too many zeros, choose alternative statistical procedure (Chapter 10)


## Example (Sign test)

Example (HWC: Chapter 3, Example 3.5, pg. 65) - paired sample sign test

- Beak-Clapping Counts.
- Subjects: chick embryos.
- $X =$ average number of claps per minute during the dark period.
- $Y =$ average number of claps per minute during the period of illumination.
- Test responsivity of a (changes in the beak-clapping constituted a sensitive indicator of auditory responsiveness.) chick embryo to a light stimulus.
- $\text{H}_{A}: \theta > 0$.

##

```{r}
df = data.frame(X = c(5.8,13.5,26.1,7.4,7.6,23,10.7,9.1,
  19.3,26.3,17.5,17.9,18.3,14.2,55.2,15.4,30,21.3,
  26.8,8.1,24.3, 21.3,18.2,22.5,31.1), 
  Y = c(5,21,73,25,3,77,59,13,36,46,9,25,
    59,38,70,36,55,46,25,30,29,46,71,31,33))
head(df)
```

##
```{r}
library(dplyr)
df = mutate(df, Z= Y-X, Psi = ifelse(Z > 0 ,1,0))
head(df)
```


## 
- `lower.tail=F` provides $P(B > b_{\alpha = .05}) = .05$

```{r}
qbinom(p = .05, size = length(df$Psi), 
  prob = 1/2, lower.tail = F)
```

- We need $P(B \geq  b) = .05$. Therefore, Reject H0 if $B \geq 18$.

However, the significance level is not .05.

```{r}
1-pbinom((18-1), size = length(df$Psi), 
  prob = 1/2, lower.tail = T)
```

##

- Use the rejection region (Reject H0 if $B \geq 18$) to make the decision.

- Observed value of test statistic is 

```{r}
sum(df$Psi)
```

- We reject in favor of $\theta > 0$ at the $\alpha = .05$ level.
- Didn't use actual $Z_{i}$.
- Actual magnitude of the $Z_{i}$’s will be necessary for distribution-free point and interval estimates of $\theta$ associated with sign test.


## Use build-in function `SIGN.test` in package `BSDA` 
```{r eval=FALSE}
library(BSDA)
SIGN.test(df$Y, df$X, alt = "greater")
```

##
```{r echo=FALSE}
library(BSDA)
SIGN.test(df$Y, df$X, alt = "greater")
```

##
- P-value may also be found using the `pbinom` command: $P\left(B \geq 21\right) = 1 - P\left(B \leq 20\right)$.

```{r}
1- pbinom((21-1), size = length(df$Psi), 
  prob = 1/2, lower.tail = T)
```
- For the large-sample approximation, compute $B^{*}$.
```{r}
B.star <- (21-25/2)/sqrt(25/4)
B.star
```

- P-value
```{r}
1-pnorm(B.star)
```

##
- Both the exact test and the large-sample approximation indicate that there is strong evidence that chick embryos are indeed responsive to a light stimulus, as measured by an increase in the frequency of beak-claps.

- To test $\text{H}_{0} = \theta_{0}$, 
    - compute $Z_{1}^{'} = Z_{1}-\theta_{0}, \cdots, Z_{n}^{'} = Z_{n}-\theta_{0}$.
    - do sign test on $Z_{i}^{'}$s.

<!-- - Sign test -->
<!-- - Homework problems pg. 74: (44, 45) -->

##  Parametric t-test

- Let $Z_{i} \sim \text{N}\left(\theta, \sigma^2\right)$.
- $\text{H}_{0}: \theta = 0$ versus $\text{H}_{A}: \theta > 0$.
- Test statistic: $T = \dfrac{\bar{Z} - \theta}{s^{2}/n}$.
- $T$ is Studentized t-distribution with degrees of freedom $n-1$.
- $t_{0}$: the observed value of test statistic.
- P-value: $P\left( T \geq t_{0}\right)$.

#  Wilcoxon signed rank test

## Wilcoxon signed rank test

- Assumptions:
    - $Z_{i} =  Y_{i} - X_{i} \sim F_{i}$, where $F_{i}$ is symmetric about common median $\theta$.
- Test statistic: $T^{+} = \sum_{i=1}^{n} R_{i}\psi_{i}$, sum of positive signed ranks.
    - no-closed form distribution.
    - use iterative algorithms.
- Rejection regions
    - $\text{H}_{A}: \theta > 0$, Reject $\text{H}_{0}$ if $T^{+} \geq t_{\alpha}$.
    - $\text{H}_{A}: \theta < 0$, Reject $\text{H}_{0}$ if $T^{+} \leq \dfrac{n(n+1)}{2}-t_{\alpha}$.
    - $\text{H}_{A}: \theta \neq 0$, Reject $\text{H}_{0}$ if $T^{+} \geq t_{\alpha/2}$ or $T^{+} \leq \dfrac{n(n+1)}{2}-t_{\alpha/2}$.

## Large-sample approximation
- Read HWC page 41 -42 and comment 7 in page 48.

## Ties
- Discard zero values among the $Z_{i}$'s.
- If there are ties, assign each of the observations in a tied group the average of the integer ranks that are associated with the tied group.
    - not exact test

##  Theoretical distribution of $T^{+}$ for $n=3$

- Comment 5 in page 46.

- Enumerate all $2^{n}$ possible outcomes for sample size three n=3:
```{r }
library(gtools)
x <- c(0,1)
df <- permutations(n=2, r=3, v= x, 
  repeats.allowed=T) %>% data.frame
df 
```

##
```{r}
T.plus = apply(df, 1, 
  function(x){sum(x%*%seq(1,3))
    })
df = mutate(df, T.plus = T.plus)
df
```

##
```{r}
table(df$T.plus)/sum(table(df$T.plus))
```

## Monte Carlo Simulation

<!-- - Approximate the distribution of $T^{+}$ using Monte Carlo simulation -->

- Compare Monte Carlo simulation results with the theoretical results:
```{r}
n = 3; nsim = 10000; Z = matrix(rnorm(n*nsim),ncol=n)
T.plus.mc = apply(Z, 1, 
  function(x) {sum(rank(abs(x)) * (x>0) ) 
    })
table(T.plus.mc)/nsim
```


## Example (Wilcoxon signed rank test)

- Data are from nine patients who received tranquilizer.
- $X$ (pre) factor IV value was obtained at the first patient visit after initiation of therapy.
- $Y$ (post) factor IV value was obtained at the second visit after initiation of therapy.
- Test improvement due to tranquilizer that corresponds to a reduction in
factor IV values.

##
```{r}
pre = c(1.83, .50, 1.62, 2.48, 1.68, 1.88, 
  1.55, 3.06, 1.30)
post = c(.878, .647, .598, 2.05, 1.06, 1.29, 
  1.06, 3.14, 1.29)
wilcox.test(post, pre, paired=TRUE, 
  alterative = "less")
```


## 
```{r}
df <- data.frame(X= pre, Y = post)
df <- mutate(df, Z = Y-X, R=rank(abs(Z)), 
  psi = ifelse(Z>0,1,0),Rpsi = R*psi)
df
```

##

P-value is $P\left(T^{+} \leq 5\right)$
```{r}
psignrank(q=sum(df$Rpsi),n=9,lower.tail = T)
```

- There is strong evidence that tranquilizer does lead to patient improvement at $\alpha =.05$, as measured by a reduction in the Hamilton scale factor IV values.

<!-- ##  -->
<!-- - Homework problems pg. 54: (4) -->

# Point and interval estimates

##
- All three tests (sign test, Wilcoxon signed rank, and t-test) have an associated estimate and confidence interval for the location parameter $\theta$.
- Order statistic:
    - $Z_{(1)} < Z_{(2)} < \cdots < Z_{(n)}$.
    - $Z_{(1)}$ is the minimum.
    - $Z_{(n)}$ is the maximum.
    
- Quantile: equally spaced splitting points of continuous intervals with equal probabilities.

## The point and interval estimate of $\theta$ associated with the sign rank statistic

- median: $\tilde{\theta} = \text{median}\left\lbrace Z_{i}, i =1, \cdots, n \right\rbrace$.
- Let $Z_{(1)}, \cdots, Z_{(n)}$ denote the ordered $Z_{i}$ and if
    - $n$ is odd, $\tilde{\theta} = Z_{(k+1)},$ where $k= (n-1)/2$.
    - $n$ is even, $\tilde{\theta} = \dfrac{Z_{(k)}+ Z_{(k+1)}}{2},$ where $k= n/2$.
- $100(1-\alpha)\%$ confidence interval associated with two-sided test:
$\left(Z_{(n+1 - b_{\alpha/2;n,1/2})},Z_{(b_{\alpha/2;n,1/2})}\right),$ $b_{\alpha/2;n,1/2}$ is the upper $\alpha/2$ percentile of the null distribution of $B$ (sign test statistic).


## The point and interval estimate of $\theta$ associated with the Wilcoxon signed rank statistic

- Hodges–Lehmann estimator: $\hat{\theta} = \text{median}\left\lbrace \dfrac{Z_{i} + Z_{j}}{2}; i \leq j = 1, \cdots, n \right\rbrace$.
    - Walsh averages $\dfrac{Z_{i} + Z_{j}}{2}; i \leq j = 1, \cdots, n$.
    - $M = \dfrac{n(n+1)}{2}$ Walsh averages.
    - $W_{(1)} \leq \cdots \leq W_{(M)}$ denote the ordered values of $\left(Z_{i} + Z_{j} \right)/2$.
- If  
    - $M$ is odd, $\hat{\theta} = W_{(k+1)},$ where $k= (M-1)/2$.
    - $M$ is even, $\hat{\theta} = \dfrac{W_{(k)}+ W_{(k+1)}}{2},$ where $k= M/2$.

##

- $100(1-\alpha)\%$ confidence interval associated with two-sided test:
$\left(W_{\left(\dfrac{n(n+1)}{2}+1-t_{\alpha/2}\right)},W_{\left(t_{\alpha/2}\right)}\right),$ $t_{\alpha/2}$ is the upper $\alpha/2$ percentile of the null distribution of $T^{+}$.
- $t_{\alpha/2}$, the percentile points can be found using the R function `psignrank`.


<!-- ##  -->
<!-- - Homework problems pg. 79: (61) -->
<!-- - Homework problems pg. 81: (71) -->
<!-- - Homework problems pg. 58: (21, 23(a,b)) -->
<!-- - Homework problems pg. 62: (28) -->


##  Relaionship between Wilcoxon signed rank test statistic and Walsh averages (Tukey (1949))
- **HWC** page 57, comment 17.
- Wilcoxon test statistic: $T^{+} = \sum_{i=1}^{n} R_{i}\psi_{i}$
- Number of Walsh averages greater than $\theta$: $W^{+} = \# \left\lbrace\dfrac{Z_{i} + Z_{j}}{2} > \theta\right\rbrace$.
- Prove $T^{+} = W^{+}$ by induction.
- Base of the Induction:
    - Assume that $\theta$ is greater than all $Z_{1}, \cdots, Z_{n}$.
        - then, $\theta$ is greater than all Walsh averages. Thus, $W^{+} = 0$.
        - then, $Z_{i} - \theta$ are all negative. Thus, $T^{+} = 0$.

<!-- 1. Under **(A)** all ranks are negative, hence $T^{+} = 0$ -->
<!-- 2. Under **(A)** all of Walsh average are less than $\theta$, hence $W^{+} = 0$ -->

##  Relaionship between Wilcoxon signed rank test statistic and Walsh averages
- Induction Steps:
    - Move $\theta$ to the left passing through $Z_{1}, \cdots, Z_{n}$ one and two at the time and show that
    - $W^{+}$ changes value when moving past an Walsh average by the same amount
    - $T^{+}$ changes value when 
        - ranks of some $|Z_{i} - \theta|$ change or
        - sign of some rank change by the same amount
- See the complete proof [here](http://www.stat.umn.edu/geyer/s06/5601/theo/wilcox.pdf).

## Comparison
- Power of a statistical test: the probability of rejecting the null hypothesis when it is false.
- The power of the sign test can be low relative to t-test.
- The power of signed-rank Wilcoxon test is nearly that of the t-test for normal distributions and generally greater than that of the t-test for distributions with heavier tails than the normal distribution.

Note: Read HWC page 71, comment 35 (power results for sign test)

## Empirical power calculation $\theta = 0$
```{r}
power.compute <- function(n = 30, 
  df = 2, 
  nsims = 1000, 
  theta = 0){
  wil.sign.rank = rep(0, nsims)
  ttest = rep(0,nsims)
  Z = matrix((rt(n*nsims,df) + theta), 
    ncol = n,nrow = nsims)
  wil.sign.rank = apply(Z, 1, function(x){
    wilcox.test(x)$p.value})
  ttest = apply(Z, 1, function(x){t.test(x)$p.value})
  pow.wil.sign.rank = mean(wil.sign.rank <=.05)
  pow.ttest = mean(ttest <=.05)
  rt = c(pow.wil.sign.rank, pow.ttest)
  names(rt) = c("Wilcoxon.signed.rank.power",
    "t.test.power")
  return(rt)
}

```

## Empirical power calculation $\theta = 0$
```{r}
power.compute.val = power.compute(n=30, df =2, 
  nsims =1000, theta = 0)
power.compute.val
```

## Empirical power calculation $\theta = 0.5$
```{r}
power.compute.val = power.compute(n=30, df =2, 
  nsims =1000, theta = 0.5)
power.compute.val
```

## Empirical power calculation $\theta = 1$
```{r}
power.compute.val = power.compute(n=30, df =2, 
  nsims =1000, theta = 1)
power.compute.val
```

## Summary

- Assumptions on $F_{i}$
    - Sign Test: any continuous distribution.
    - Signed-Rank Test: any symmetric continuous distribution.
    - t-test: any normal distribution.

- The continuity assumption assures that ties are impossible: With probability one we have $Z_{i} \neq Z_{j}$ when $i \neq j$.

- The continuity assumption is only necessary for exact hypothesis tests not for estimates and confidence intervals.


# References

##  References for this lecture


HWC: Chapter 3.4-3.6, 3.8, 3.1-3.3, 3.7

KM: Chapter 2, page 21, Example 2.3.2. (empirical power)


