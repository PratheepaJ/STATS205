---
title: "Lecture 15: Nonparemetric regression I"
shorttitle: "STATS 205 Lecture 15"
author: "Pratheepa Jeganathan"
date: "05/06/2019"
output: 
  beamer_presentation:
    colortheme: "seahorse"
    slide_level: 2
    includes:
      in_header: header.tex
bibliography: nonparamterics.bib

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width = 5, fig.height = 5, fig.align="center",message=FALSE, warning=FALSE, out.width = '70%')
```

#  Recall

##
- One sample sign test, Wilcoxon signed rank test, large-sample approximation, median, Hodges-Lehman estimator, distribution-free confidence interval.
- Jackknife for bias and standard error of an estimator.
- Bootstrap samples, bootstrap replicates.
- Bootstrap standard error of an estimator.
- Bootstrap percentile confidence interval.
- Hypothesis testing with the bootstrap (one-sample problem.)
- Assessing the error in bootstrap estimates.
- Example: inference on ratio of heart attack rates in the aspirin-intake group to the placebo group.
- The exhaustive bootstrap distribution.


##
- Discrete data problems (one-sample, two-sample proportion tests, test of homogeneity, test of independence).
- Two-sample problems (location problem - equal variance, unequal variance, exact test or Monte Carlo, large-sample approximation, H-L estimator, dispersion problem, general distribution).
- Permutation tests (permutation test for continuous data, different test statistic, accuracy of permutation tests).
- Permutation tests (discrete data problems, exchangeability.)
- Rank-based correlation analysis (Kendall and Spearman correlation coefficients.)
- Rank-based regression (straight line, multiple linear regression, statistical inference about the unknown parameters, nonparametric procedures - does not depend on the distribution of error term.)
- Smoothing (density estimation, bias-variance trade-off, curse of dimensionality)


# Nonparametric regression

## Introduction
- Smoothers use external functions to model the functional relationship between $y$ and $x$.
- External functions: lines or low order polynomial functions.
- Nonparametric 
    - lack of a specific, parametric form assumed for the regression function being estimated.
    - no strong distributional assumptions on the errors.
- We will discuss the linear smoothers.
    - estimates are linear combinations of observed data.
- Local averaging, local regression, local polynomial, kernel smoothing, penalized regression.

## Nonparametric regression

- We are given $n$ pairs of observations $\left(x_{1}, Y_{1} \right), \cdots, \left(x_{n}, Y_{n} \right)$.
- Regression model
    - $Y_{i} = r\left(x_{i}\right) + \epsilon_{i}, i=1,\cdots, n.$
        - $Y$ response variable.
        - $x$ covariate/feature.
        - $\E \left(\epsilon_{i} \right) = 0$.
        - $r$ is a regression function.
- Estimation
    - Assume covariate value $x_{i}$ are fixed.
        - If we treat $x_{i}$ as \rc random \bc:
        - \rc Data: $\left(X_{1}, Y_{1} \right), \cdots, \left(X_{n}, Y_{n} \right)$.
        - $r\left(x_{i}\right) = \E \left( Y|X =x\right)$, mean of $Y$ conditional on $X = x$. \bc
    - Assume $\V \left(\epsilon_{i}  \right) = \sigma^{2}$ does not depend on $x$.
    - Estimate of $r\left(x\right)$ is $\hat{r}_{n}\left(x\right)$, smoother.

# Linear smoother

## Linear smoother
- Linear smoothers: estimates are linear combinations of observed data.
- An estimator $\hat{r}_{n}$ of $r$ is a linear smoother if, for each $x$, $\exists$ a vector $\vl\left(x \right) = \left( l_{1}\left(x\right), \cdots, l_{n}\left(x\right)\right)^{T}$ such that $$\hat{r}_{n}\left(x\right) = \sum_{i=1}^{n}l_{i}\left(x\right)Y_{i}.$$

- Define the vector of fitted values $$\vr = \left(\hat{r}_{n}\left(x_{1}\right), \cdots, \hat{r}_{n}\left(x_{n}\right) \right)^{T}.$$
- It follows that $$\vr = \mL \vY,$$ where $\vY = \left(Y_{1}, \cdots, Y_{n} \right)^{T}$ and $L_{ij} = l_{j}\left(x_{i}\right), i=j=1,\cdots,n$ and $\mL$ is an $n \times n$ matrix.

## Linear smoother
- The $i$-th row in $\mL$ is the weights given to each $Y_{i}$ in forming the estimate $\hat{r}_{n}\left(x_{i}\right)$.
- $L$ is called the smoothing matrix or the hat matrix.
- The $i$-th row of $\mL$ - effective kernel for estimating $r \left(x_{i}\right)$.
- $\nu = \text{tr} \left( \mL \right)$ - effective degrees of freedom.
- For all $x$, $\sum_{i=1}^{n}l_{i}\left(x \right) = 1$ (i.e., if $Y_{i} =c \hspace{.1in} \forall i$, then $\hat{r}_{n}\left(x\right) = c$.)

# Some linear smoothers

## Regressogram 


- From **W2006**.
- Mostly like histogram.
- Suppose $a \leq x \leq b, \hspace{.1in} i=1, \cdots, n.$
- Dived $\left(a,b \right)$ in to $B_{1}, \cdots, B_{m}$ equally spaced bins.
- Let $k_{j}$ be number of points in $B_{j}$. $\hat{r}_{n}$ is obtained by averaging $Y_{i}$'s over each bin. $$\hat{r}_{n}\left(x\right)= \dfrac{1}{k_{j}}\sum_{i:x_{i}\in B_{j}}Y_{i} \hspace{.1in} for \hspace{.1in} x \in B_{j}.$$
- We can write $\hat{r}_{n}\left(x\right) = \sum_{i=1}^{n}l_{i}\left(x\right)Y_{i}.$

## Regressogram

- From **W2006** Page 67, 5.24 Example.
- Example: Let $n=9, m= 3$ and $k_{1}=k_{2}=k_{3}=3.$ Then,
\begin{equation}
\notag
\mL = \dfrac{1}{3} \times
\begin{pmatrix}
1 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\
1 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\
1 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 1 & 1 & 1 & 0 & 0 & 0 \\
0 & 0 & 0 & 1 & 1 & 1 & 0 & 0 & 0 \\
0 & 0 & 0 & 1 & 1 & 1 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 1 & 1 & 1 \\
0 & 0 & 0 & 0 & 0 & 0 & 1 & 1 & 1 \\
0 & 0 & 0 & 0 & 0 & 0 & 1 & 1 & 1 
\end{pmatrix}
\end{equation}
- Effective number of freedom $\nu = \text{tr}\left(\mL\right)$.
- Binwidth $h = \dfrac{b-a}{m}$ controls the smoothness of the estimate.

## Regressogram

- **HWC** (Page 662) Example 14.2: Nitrogen Oxide Concentrations
    - Brinkman (1981) collected data on the nitrogen oxide concentrations ($\vY$) found in engine exhaust for ethanol engines with various equivalence ratios ($\vx$).

```{r fig.show='hide'}
library(NSM3)
data("ethanol")
library(HoRM)
library(ggplot2)
p = regressogram(ethanol$E, ethanol$NOx, 
  nbins = 20, show.bins = TRUE,
			 show.means = TRUE, show.lines = TRUE, 
  x.lab = "E", y.lab = "NOx", 
  main = "NOx and ethanol engines metric") + 
  theme(plot.title = element_text(hjust = 0.5))
```

## Regressogram
```{r echo=FALSE}
p
```

- blue dots are bins.

## Local averaging (Friedman)

- **HWC** Chapter 14.1
- Estimate of $r$ at the point $x_{i}$ is taken to be the average of observed values $Y_{j}$ corresponding to values $x_{j}$ in some vicinity of $x_{i}$.
- The neighborhood of $x_{i}$ is chosen to be the smallest symmetric window about $x_{i}$ containing fixed number of observations.
- The average is a linear combination of the points in the neighborhood, thus, the fit is a linear smoother.
- `supsmu` function in R.

## Local averaging 
- From **W2006** Page 68, 5.26 Example.
- For $h >0$ and let $B_{x} = \left\lbrace i: \left| x_{i} -x\right| \leq h \right\rbrace$.
- Let $n_{x}$ be the number of points in $B_{x}$.
- $\hat{r}_{n}\left(x\right) = \dfrac{1}{n_{x}}\sum_{i \in B_{x}}Y_{i}$.
- We can write \begin{equation}
\label{localAvg}
\hat{r}_{n}\left(x\right) = \sum_{i=1}^{n}l_{i}\left(x\right)Y_{i},
\end{equation}
where $l_{i}\left(x\right)=\dfrac{1}{n_{x}}$ if$\left| x_{i} -x\right| \leq h$ and 0 otherwise.
- Example: Suppose $n =9$ , $x_{i} = \dfrac{i}{9}$ and $h = \dfrac{1}{9}$. 

## Local averaging 

- \begin{equation}
\notag
\mL = 
\begin{pmatrix}
1/2 & 1/2 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
1/3 & 1/3 & 1/3 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 1/3 & 1/3 & 1/3 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 1/3 & 1/3 & 1/3 &  & 0 & 0 & 0 \\
0 & 0 & 0 & 1/3 & 1/3 & 1/3 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 1/3 & 1/3 & 1/3 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 1/3 & 1/3 & 1/3 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 1/3 & 1/3 & 1/3 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 1/2 & 1/2 
\end{pmatrix}
\end{equation}

## Local averaging 

```{r fig.show='hide'}
fit.local.avg = supsmu(ethanol$E, 
  ethanol$NOx, span = "cv")
df.fit.local.avg = data.frame(E.fit = fit.local.avg$x, 
  NOx.fit = fit.local.avg$y)
library(dplyr)
p = ggplot() + 
  geom_point(data = ethanol, aes(x = E, y=NOx)) +
  geom_line(data = df.fit.local.avg, 
    aes(x = E.fit, y = NOx.fit), color = "red")
```

## Local averaging 
```{r echo=FALSE}
p
```

## Local regression (Cleveland)

- **HWC** Chapter 14.2
- Estimate $r$ by performing a local linear regression (locally weighted least squares) on the observations $\left(x, Y\right)$ near $x_{i}$. 
- The regression is a weighted regression - weights are related to the distance of the points used in the regression to the point $x_{i}$.
- `loess` function in R, `loess.as` for cross-validation and finding optimal span.
    - The weight function used in `loess` is tricube function: $$W\left(x\right) = \left(1 - \left| x\right|^{3} \right)^{3}, \left| x\right| < 1.$$
    - Let $w_{1}^{i}, \cdots,w_{n}^{i}$ be the weights determined by the centered and scaled $W$ for a particular point $x_{i}$.
    - The weighted local regression is found by minimizing $$\sum_{j=1}^{n}w_{j}^{i}\left(Y_{i} - \beta_{0}^{i} - \beta_{1}^{i}x_{j} \right)^2.$$
    - $\beta_{0}^{i}$ and $\beta_{1}^{i}$ are the intercept and slope of the linear relation between $x$ and $Y$ in the neighborhood of $x_{i}$.

## Local regression
```{r fig.show='hide'}
fit.local.lin.reg = loess(NOx ~ E, data=ethanol, 
  degree=1, span=0.19)

df.fit.local.lin.reg = data.frame(E = ethanol$E, 
  NOx.fit = fit.local.lin.reg$fitted)
library(dplyr)
p = ggplot() + 
  geom_point(data = ethanol, aes(x = E, y=NOx)) +
  geom_line(data = df.fit.local.lin.reg, 
    aes(x = E, y = NOx.fit), color = "red")
```

## Local regression
```{r echo=FALSE}
p
```


## Local polynomial regression 

- **HWC** comment 8, page 665.
- Use local polynomial regression in place of linear regression.
- Let $w_{1}^{i}, \cdots,w_{n}^{i}$ be the weights determined by the centered and scaled $W$ for a particular point $x_{i}$.
    - The weighted local polynomial regression is found by minimizing $$\sum_{j=1}^{n}w_{j}^{i}\left(Y_{i} - \beta_{0}^{i} - \beta_{1}^{i}x_{j}- \cdots - \beta_{d}^{i}x_{j}^{d} \right)^2.$$
- `loess` function in R allows for degrees of $d = 0, 1, 2$.

## Local polynomial regression 

```{r fig.show='hide'}
fit.local.poly.reg = loess(NOx ~ E, data=ethanol, 
  degree=2, span=0.2)

df.fit.local.poly.reg = data.frame(E = ethanol$E, 
  NOx.fit = fit.local.poly.reg$fitted)

p = ggplot() + 
  geom_point(data = ethanol, aes(x = E, y=NOx)) +
  geom_line(data = df.fit.local.poly.reg, 
    aes(x = E, y = NOx.fit), color = "red")
```


## Local polynomial regression
```{r echo=FALSE}
p 
```

<!-- ## Local regression with Kernels -->

<!-- - **W 2006** Page 77, Theorem 5.60 -->
<!-- - Local linear regression. -->
<!-- - $\hat{r}_{n}\left(x \right) = \sum_{i=1}^{n}l_{i}\left(x \right)Y_{i},$ where $$l_{i}\left(x \right) = \dfrac{b_{i}\left(x \right)}{\sum_{j=1}^{n}b_{j}\left(x \right)},$$  -->
<!-- where \footnotesize {$$b_{i}\left(x \right)= \left(\sum_{i=1}^{n}K\left(\dfrac{x_{i}-x}{h} \right)\left( x_{i}-x\right)^{2} -  \left( x_{i}-x\right)\sum_{i=1}^{n}K\left(\dfrac{x_{i}-x}{h} \right)\left( x_{i}-x\right) \right).$$} -->


<!-- ## Local polynomial regression with Kernels -->

<!-- - **W 2006** Chapter 5.4, Page 76, Theorem 5.57. -->
<!-- - $\hat{r}_{n}\left(x \right) = \sum_{i=1}^{n}l_{i}\left(x \right)Y_{i},$ where $\vl\left(x\right) = \left(l_{1}\left(x\right), \cdots, l_{n}\left(x\right) \right)^{T},$ $$l\left(x\right)^{T} = \ve_{1}^{T}\left(\mX^{T}_{x} \mW_{x} \mX_{x}\right)^{-1}\mX^{T}_{x} \mW_{x},$$ where $\ve_{1} =\left(1, 0 \cdots, 0 \right)^{T}.$ -->


## Kernel smoothing

- **HWC** Chapter 14.3.
- This is not a nearest neighbor method for a given kernel $K$ and bandwidth $h$.
    - The number of observations used in the estimate at any point $x$ is not fixed but the window size is.
    - The bandwidth $h$ is the changing value over which the least squares are minimized, rather than the span.
    - Weights are determined by how close each observation $x_{j}$ to point $x$, bandwidth, and the kernel $K$.
- `npreg` from package `np`.
    - `npregbw` from package `np` for bandwidth selection.

## Kernel smoothing

- **W 2006** Chapter 5.4.
- Let $h >0$ - bandwidth.
- Nadaraya (1964, 1965) and Watson (1964): \begin{equation} 
\label{kersmooth}
\hat{r}_{n}\left(x \right)=\sum_{i=1}^{n}l_{i}\left( x\right)Y_{i},\end{equation} where $K$ is a kernel and $$l_{i}\left( x\right)=\dfrac{K\left(\dfrac{x-x_{i}}{h}\right)}{\sum_{j=1}^{n}K\left(\dfrac{x-x_{j}}{h}\right)}.$$
- The local average regression in \eqref{localAvg} is a kernel estimator based on the boxcar kernel.
- We can show that kernel smoother is a linear smoother as in \eqref{kersmooth}. 

## Kernel smoothing

- The choice of kernel $K$ is not too important.
- Risk is sensitive for $h_{n}$ which controls the amount of smoothing and depends on sample size $n$.
    - Small $h_{n}$ gives rough estimates.
    - Larger $h_{n}$'s give smoother estimates.

## Kernel smoothing
- An example to show the bandwidth affects the estimate.
- Let $x_{1}, x_{2}, \cdots, x_{n}$ be random draws from some density $f$.
- The risk (integrated squared error loss) of the Nadaraya-Watson kernel estimator is \begin{equation}
\label{example_bandwidth_risk}
\begin{split}
R\left( \hat{r}_{n}, r\right) &= \dfrac{h_{n}^{4}}{4}\left(\int x^{2}K\left(x\right) dx\right)^{2}\int \left(r^{''}\left(x\right)+ 2 r^{'}\left(x\right)\dfrac{f^{'}\left(x\right)}{f\left(x\right)} \right)^{2}dx \\
& + \dfrac{\sigma^{2}\int K^{2}\left(x\right)dx}{nh_{n}}\int \dfrac{1}{f\left(x\right)}dx+ o\left(nh_{n}^{-1}\right) + o\left(h_{n}^{4}\right)
\end{split}
\end{equation}
as $h_{n} \to 0$ and $nh_{n} \to \infty$.

- **Design bias**: $2 r^{'}\left(x\right)\dfrac{f^{'}\left(x\right)}{f\left(x\right)}$ The bias term in \eqref{example_bandwidth_risk} depends on the distribution of $x_{i}$'s.


## Kernel smoothing
- The optimal bandwidth will depend on the unknown function $r$. So we can use cross-validation to find the optimal bandwidth $h^{*}$.

## Kernel smoothing

- Kernel estimators have high bias near the boundaries called **boundary bias**.

![Boundary bias in kernel estimators](boundary_bias_kernel.png)

## Kernel smoothing

- Alleviate the boundary bias and design bias using local polynomial regression.
    - Use the kernel $K$ as the weight in the local polynomial regression.
    - Estimate is a linear smoother.

## Kernel smoothing

```{r}
library(np)
ethanol.npreg <- npreg(bws=.09, 
  txdat=ethanol$E,
     tydat=ethanol$NOx, 
  ckertype="epanechnikov")
ethanol.npreg2 <- npreg(bws=.03, 
  txdat=ethanol$E,
     tydat=ethanol$NOx, 
  ckertype="epanechnikov")
ethanol.npreg$MSE
ethanol.npreg2$MSE
```


## Kernel smoothing

```{r fig.show='hide'}
ethanol.npreg.fit = data.frame(E = ethanol$E, 
  NOx = ethanol$NOx, 
  kernel.fit = fitted(ethanol.npreg), 
  kernel.fit2 = fitted(ethanol.npreg2))

ggplot(ethanol.npreg.fit) + 
  geom_point(aes(x = E, y = NOx)) + 
  geom_line(aes(x = E, y= kernel.fit), color = "red") + 
  geom_line(aes(x = E, y= kernel.fit2), color = "blue") 
```

## Kernel smoothing

```{r echo=FALSE}
ggplot(ethanol.npreg.fit) + 
  geom_point(aes(x = E, y = NOx)) + 
  geom_line(aes(x = E, y= kernel.fit), color = "red") + 
  geom_line(aes(x = E, y= kernel.fit2), color = "blue") 
```

## Penalized regression

- **W2006** Chapter 5.5
- $Y_{i} = r\left(x_{i}\right) + \epsilon_{i}.$
- Suppose we estimate $r$ by choosing $\hat{r}_{n}\left(x\right)$ to minimize the sum of squares $$\sum_{i=1}^{n} \left( Y_{i}- \hat{r}_{n}\left(x\right) \right)^{2}.$$
    - Minimizing over all linear functions gives least squares estimator.
    - Minimizing over all functions yields a function that interpolate the data.
- To avoid the above two extreme solutions
    - locally weighted sums of squares (local averages, local linear/polynomial regression, kernel smoother).
    - minimize the penalized sums of squares.
    
## Penalized regression

- Compute $\hat{r}_{n}$ by minimizing penalized sums of squares $$M\left( \lambda\right) = \underset{i}{\sum}\left( Y_{i} -\hat{r}_{n}\left(x_{i}\right) \right)^{2} + \lambda J\left(r\right),$$ where $$J\left(r\right) = \int \left(r^{''}\left(x\right)^{2} dx \right).$$
- When $\lambda =0$, the solution is interpolating function.
- When $\lambda \to \infty$, $\hat{r}_{n}$ converges to the least squares line.
- What does $\hat{r}_{n}$ looks like for $0 < \lambda < \infty$?

## Splines

- A spline is a special piece-wise polynomial.
- A cubic spline
    - Let $\zeta_{1}, \zeta_{2}, \cdots,\zeta_{k}$ be a set of ordered points - called knots - contained in some interval $\left(a,b \right)$.
    - A cubic spline is a continuous function $r$ such that (i) $r$ is a cubic polynomial over $\left(\zeta_{1}, \zeta_{2}\right), \cdots$ and (ii) $r$ has first and second derivatives at knots.


## Smoothing splines

- The function $\hat{r}_{n}\left(x \right)$ that minimizes $M\left( \lambda\right)$ with penalty $J\left(r\right)$ is a natural cubic spline with knots at the data points.
    - $\hat{r}_{n}$ does not have an explicit form.
    - Smoothing splines.
- Build an explicit basis using B-splines $$\hat{r}_{n}\left(x \right) =\sum_{j=1}^{N}\hat{\beta}_{j}B_{j}\left(x \right),$$
    - where $B_{1}, \cdots, B_{N}$ are a basis for B-splines with $N= n+4$.
    - Now we only need to find the coefficients $\hat{\vbeta} = \left(\hat{\beta}_{1}, \cdots, \hat{\beta}_{N} \right)^{T}$.
    
## B-Splines

- By expanding $r$ in the basis we can now rewrite the minimization as follows: $$\text{minimize}\left(Y - \mB \vbeta \right)^{T} \left( Y - \mB \vbeta \right) + \lambda \vbeta^{T} \Omega \vbeta,$$ where $\mB_{ij} = B_{j} \left( X_{i} \right)$ and $\Omega_{ij} = \int B_{j}^{''}\left(x\right) B_{k}^{''}\left(x\right)dx.$
    - $$\hat{\vbeta} = \left(\mB^{T} \mB + \lambda \Omega \right)^{-1} \mB^{T} Y.$$
- The smoothing spline is a linear smoother: $$\vr = \left(\mB^{T} \mB + \lambda \Omega \right)^{-1} \mB^{T} \vY = \mL \vY.$$ 

## B-Splines

- Cubic B-spline basis using nine equally spaced knots on (0,1).

```{r echo=FALSE}
library(png)
library(grid)
img <- readPNG("cubic_B_spline.png")
grid.raster(img)
```

## Splines (Example)
```{r}
library(splines)
```

- A Cubic Spline with 3 Knots 
```{r}
range(ethanol$E)
cubic.spline.fit = lm(NOx ~ bs(E, 
  knots = c(.75,1,1.2)), 
  data = ethanol)
```

## Splines (Example)
```{r}
summary(cubic.spline.fit)
```

## Splines (Example)
```{r fig.show='hide'}
df.cubic.spline = data.frame(x = ethanol$E, 
  y = ethanol$NOx, 
  fit = fitted(cubic.spline.fit))
p = ggplot(data = df.cubic.spline) + 
  geom_point(aes(x = x, y = y)) +
  geom_line(aes(x = x, y = fit), 
    color = "red") +
  geom_vline(xintercept = c(.75,1,1.2), 
    color = "green")

```

## Splines (Example)
```{r echo=FALSE}
p
```

## Smoothing spline (Example)

```{r fig.show='hide'}
smooth.spline.fit =smooth.spline(ethanol$E, 
  ethanol$NOx, cv = TRUE)

df.smooth.splines = data.frame(x = smooth.spline.fit$x,
  fit.smooth.spline = smooth.spline.fit$y)

p = ggplot() + 
  geom_point(data = df.cubic.spline, 
    aes(x = x, y = y)) +
   geom_line(data = df.cubic.spline, 
     aes(x = x, y = fit), color = "red") +
    geom_line(data = df.smooth.splines, 
      aes(x = x, y = fit.smooth.spline), 
      color = "brown") 
```

## Smoothing spline (Example)

```{r echo=FALSE}
p
```


##  References for this lecture

**HWC** Chapter 14 (smoothing)

**W** Chapter 5 

<!-- Homework assignment 6 Page 661, Problem 1 (local averaging) -->

<!-- Homewoork assignment 6 Page 661, Problem 2 (local averaging and choosing optimal span). -->

<!-- Consider the data set sunspots from Andrews and Herzberg (1985) as a response variable. For the predictor data x, use -->
<!--             x <- c(1:length(sunspots)) -->
<!-- Apply Friedmanâ€™s smoother using trial and error to find a span that seems to work well with the data. Then find an estimate using the span determined by cross-validation. Describe the results (taking into account Comment 5). -->
