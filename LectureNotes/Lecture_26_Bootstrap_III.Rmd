---
title: "Lecture 26: Bootstrap III"
shorttitle: "STATS 205 Lecture 26"
author: "Pratheepa Jeganathan"
date: "06/03/2019"
output: 
  beamer_presentation:
    colortheme: "seahorse"
    slide_level: 2
    includes:
      in_header: header.tex
bibliography: nonparamterics.bib

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width = 5, fig.height = 5, fig.align="center",message=FALSE, warning=FALSE, out.width = '70%')
```

#  Recall

##
- One sample sign test, Wilcoxon signed rank test, large-sample approximation, median, Hodges-Lehman estimator, distribution-free confidence interval.
- Jackknife for bias and standard error of an estimator.
- Bootstrap samples, bootstrap replicates.
- Bootstrap standard error of an estimator.
- Bootstrap percentile confidence interval.
- Hypothesis testing with the bootstrap (one-sample problem.)
- Assessing the error in bootstrap estimates.
- Example: inference on ratio of heart attack rates in the aspirin-intake group to the placebo group.
- The exhaustive bootstrap distribution.


##
- Discrete data problems (one-sample, two-sample proportion tests, test of homogeneity, test of independence).
- Two-sample problems (location problem - equal variance, unequal variance, exact test or Monte Carlo, large-sample approximation, H-L estimator, dispersion problem, general distribution).
- Permutation tests (permutation test for continuous data, different test statistic, accuracy of permutation tests).
- Permutation tests (discrete data problems, exchangeability.)
- Rank-based correlation analysis (Kendall and Spearman correlation coefficients.)
- Rank-based regression (straight line, multiple linear regression, statistical inference about the unknown parameters, nonparametric procedures - does not depend on the distribution of error term.)
- Smoothing (density estimation, bias-variance trade-off, curse of dimensionality)
- Nonparametric regression (Local averaging, local regression, kernel smoothing, local polynomial, penalized regression)

## 
- Cross-validation, Variance Estimation, Confidence Bands, Bootstrap Confidence Bands.
- Wavelets (wavelet representation of a function, coefficient estimation using Discrete wavelet transformation, thresholding - VishuShrink and SureShrink).
- One-way layout (general alternative (KW test), ordered alternatives), multiple comparison procedure.
- Two-way layout (complete block design (Friedman test)), multiple comparison procedure, median polish, Tukey additivity plot, profile plots.
 
# Better bootstrap confidence intervals

## Overview

- We learned
    - Plug-in principal
    - Computing standard error of an estimate
    - Confidence intervals based on bootstrap percentiles 
        - Coverage performance (need to do)
    - Hypothesis testing using bootstrap
        - Using bootstrap percentile confidence interval.
        - Using p-value.
    - Exhaustive bootstrap.
- What to cover
    - bootstrap-t interval
    - BCa interval and ABC interval.
    
## Problem
- Inference on one parameter. 
- Let  $\vx = \lbrace x_{1}, \cdots, x_{n}\rbrace \sim F_{\theta}\left(\cdot\right),$ where $\theta$ is an unknown parameter.
- Construct $1 - 2\alpha$ confidence interval for $\theta$.

## Confidence interval
- Suppose $$\hat{\theta} \sim \text{N}\left(\theta, \text{se}^{2} \right).$$
- Then, $$Z = \dfrac{\hat{\theta}-\theta}{ \text{se}} \sim \text{N}\left(0, 1 \right).$$
- Thus, $$\text{Prob}_{\theta}\left\lbrace \theta \in \left[\hat{\theta} - z^{\left(1-\alpha \right)}\text{se}, \hspace{.1in} \hat{\theta} - z^{\left(\alpha \right)}\text{se}\right] \right\rbrace = 1 -2\alpha,$$ where $\text{Prob}_{\theta}\lbrace \rbrace$ is the probability calculated with the true mean equaling $\theta$, so $\hat{\theta} \sim \text{N}\left(\theta, \text{se}^{2} \right)$.

## Coverage of confidence interval

- In the above case, $$\hat{\theta} \sim \text{N}\left(\theta, \text{se}^{2} \right),$$ the interval $$\left[\hat{\theta} - z^{\left(1-\alpha \right)}\text{se}, \hspace{.1in} \hat{\theta} - z^{\left(\alpha \right)}\text{se}\right]$$ has probability exactly $1 - 2 \alpha$ of containing the true value of $\theta$.
    - $\theta$ is a constant.
    - Let $\hat{\theta}_{\text{lo}} = \hat{\theta} - z^{\left(1-\alpha \right)}\text{se}$ and $\hat{\theta}_{\text{up}} = \hat{\theta} - z^{\left(\alpha \right)}\text{se}$. Then, $\hat{\theta}_{\text{lo}}$ and $\hat{\theta}_{\text{up}}$ are random variables.
- equal-tailed CI: If $\text{Prob}_{\theta}\lbrace \theta <  \hat{\theta}_{\text{lo}}\rbrace = \alpha$ and $\text{Prob}_{\theta}\lbrace \theta > \hat{\theta}_{\text{up}} \rbrace = \alpha$, then $\left(\hat{\theta}_{\text{lo}}, \hat{\theta}_{\text{up}} \right)$ is an equal-tailed.

## Relationship between confidence intervals and hypothesis tests
- $1 - 2\alpha$ confidence interval $\left(\hat{\theta}_{\text{lo}}, \hat{\theta}_{\text{up}} \right)$ is the set of plausible values of $\theta$ having observed $\hat{\theta}$.
- Check whether the null value is in the interval.
    - If the null value of $\theta$ is not in the interval, reject the null hypothesis.


## Standard confidence interval (procedure)

- In most cases, $$\dfrac{\hat{\theta} - \theta}{\hat{\text{se}}} \overset{\cdot} \sim \text{N}\left(0, 1 \right)$$.
    - $1 - 2\alpha$ standard confidence interval for $\theta$ is $$\hat{\theta} \pm z^{\left(1-\alpha \right)}\hat{\text{se}},$$ where $z^{\alpha}$ is the $100 \cdot \alpha$ the percentile point of $\text{N}\left(0, 1 \right)$.
    
```{r}
## z^{.05}
qnorm(.05)
## z^{.95}
qnorm(.95)
```

    
## Standard confidence interval (Example)

- **ET** Table 2.1.
- 16 mice were randomly assigned to a treatment or a control group. 
- Their survival time in days, following a surgery was recorded.
- Construct 90\% confidence interval for the expectation $\theta$ of the control group distribution.

```{r}
Table2.1.ET = list(treatment = c(94, 197, 
  16, 38, 99, 141, 23), 
  control = c(52, 104, 146, 10, 
    51, 30, 40, 27, 46))
```

## Standard confidence interval (Example)

$1 -2 \alpha = .9$. Thus, $\alpha = .05$.
```{r}
x = Table2.1.ET$control
n = length(x)
theta.hat = round(mean(x), digits = 2)
theta.hat
se.theta.hat = round(sd(x)/sqrt(n), digits = 2)
se.theta.hat
## z^{1-.05}
z = round(qnorm(.95), digits = 3)
z
```

## Standard confidence interval (Example)
- 90\% confidence interval for the expectation $\theta$ of the control group distribution is
```{r}
ci.standard = round(theta.hat + c(-1, 1)*z*se.theta.hat, 
  digits = 2); ci.standard
```

- 90\% of time, a random interval constructed in this way will contain the true value $\theta$.

## Standard confidence interval (Note)
- $\dfrac{\hat{\theta} - \theta}{\hat{\text{se}}} \overset{\cdot} \sim \text{N}\left(0, 1 \right)$ is valid as $n \to \infty$, but is approximation for finite samples. 
- Thus, for the example with $n =9$, actually the standard CI is an approximate CI.
    - The coverage probability is not exactly $1 - 2 \alpha$.

## Student's t-interval (procedure)
- Improve upon the standard confidence interval. $$Z = \dfrac{\hat{\theta} - \theta}{\hat{\text{se}}} \overset{\cdot} \sim \text{t}_{n-1},$$ where $\text{t}_{n-1}$ is the Student's $t$ distribution on $n-1$ degrees of freedom.
- Student's t-interval is $$\left[\hat{\theta} - t_{n-1}^{\left(1-\alpha \right)}\hat{\text{se}}, \hspace{.1in} \hat{\theta} - t_{n-1}^{\left(\alpha \right)}\hat{\text{se}}\right].$$


## Student's t-interval (Example)
```{r}
##t^{.05}
qt(.05, df = 8)
##t^{.95}
qt(.95, df = 8)
```

## Student's t-interval (Example)

- 90\% confidence interval for the expectation $\theta$ of the control group distribution is
```{r}
ci.student.t = round(theta.hat + c(qt(.05, df = 9), 
  qt(.95, df = (length(x)-1)))*se.theta.hat, 
  digits = 2)
ci.student.t
```

- Student's t-interval is wider than the standard interval
```{r}
ci.standard
```

## Student's t-interval (Note)

- Student's t-interval widening the interval to adjust for the fact that the standard error is unknown.

## Student's t-interval (Note)

- Increase $n (\geq 20)$, percentiles of $t_{n}$ distribution don't differ much from the standard normal $\text{N}\left(0,1 \right)$.
```{r}
##(t^{.05}, t^{.95})
c(qt(.05, df = 50), qt(.95, df = 50))

## (z^{.05},  z^{.95})
c(qnorm(.05), qnorm(.95))
```


## Student's t-interval (Note)
- The use of the $t$ distribution doesn't adjust the CI to account for skewness in the underlying population or other errors when $\hat{\theta}$ is not the sample mean (for example, bias of an estimate).

## The bootstrap-t interval (overview)
- Adjust for the above errors.
- Construct CI without having $$Z = \dfrac{\hat{\theta} - \theta}{\hat{\text{se}}} \overset{\cdot} \sim \text{N}\left(0, 1 \right) \hspace{.1in}  \text{or} \hspace{.1in}  Z = \dfrac{\hat{\theta} - \theta}{\hat{\text{se}}} \overset{\cdot} \sim \text{t}_{n-1}$$.
- Estimate the distribution of $Z$ directly from the data.

## The bootstrap-t interval (procedure)
- The bootstrap-$t$ method
    - Generate $B$ bootstrap samples $\vx^{*1}, \vx^{*2}, \cdots, \vx^{*B}$.
    - For each compute $$Z^{*}\left(b\right) = \dfrac{\hat{\theta}^{*}-\hat{\theta}}{\hat{\text{se}}^{*}\left(b\right)},$$ where 
        - $\hat{\theta}^{*}$ is the value of $\hat{\theta}$ for the bootstrap sample $\vx^{*b}$ 
        - $\hat{\text{se}}^{*}\left(b\right)$ is the estimated standard error of $\hat{\theta}^{*}$.
    - Let $k$ be the largest integer less than or equal to $\left(B+1 \right)\alpha.$
        -  $\hat{\text{t}}^{(1-\alpha)}$ - the empirical $\alpha$ quantile is the $k$-th largest value of $Z^{*}\left(b\right)$.
        - $\hat{\text{t}}^{(\alpha)}$ - the empirical $1-\alpha$ quantile is the $\left(B+1-k\right)$-th largest value of $Z^{*}\left(b\right)$.
    - The bootstrap-$t$ confidence interval is $$\left[\hat{\theta} - \hat{\text{t}}^{(1-\alpha)}\hat{\text{se}}, \hspace{.1in} \hat{\theta} - \hat{\text{t}}^{(\alpha)}\hat{\text{se}}\right].$$
        
## The bootstrap-t interval (Example)

- 90\% Bootstrap-t interval for the expectation $\theta$ of the control group.
- $\alpha = .05$.
```{r}
B = 1000
n = length(x)
theta.hat = mean(x); theta.hat
se.theta.hat = sd(x)/sqrt(n); se.theta.hat
```

## The bootstrap-t interval (Example)
```{r}
z.star = function(x){
  n = length(x)
  x.star = sample(x, size = n, 
    replace = TRUE)
  theta.hat.star = mean(x.star)
  se.theta.hat.star = sd(x.star)/sqrt(n)
  z.star.b = (theta.hat.star - 
      theta.hat)/(se.theta.hat.star)
  return(z.star.b)
}
z.star.B = replicate(B, z.star(x))
```

## The bootstrap-t interval (Example)
```{r}
#k is the largest integer less than 
#or equal to (B+1) * alpha.
alpha = .05
k = ceiling((B+1)*alpha)
t.hat.one.minus.alpha = sort(z.star.B, 
  decreasing = TRUE)[k]
t1 = t.hat.one.minus.alpha; t1
k.u = B+1-k
t.hat.alpha = sort(z.star.B, 
  decreasing = TRUE)[k.u]
t2 = t.hat.alpha; t2
```

## The bootstrap-t interval (Example)

- 90\% confidence interval for the expectation $\theta$ of the control group distribution is
```{r}
ci.bootstrap.t = round(theta.hat - 
    c(t1, t2)*se.theta.hat, digits = 2)
ci.bootstrap.t
```

- The lower end point is close to the standard interval, 
```{r}
ci.standard
```
    - but upper end point is much greater (reflect the two very large values 104 and 146 in the data).


## The bootstrap-t interval (Note)
- For large samples, the coverage of the bootstrap-$t$ interval tends to be closer to the desired interval than the coverage of the standard and Student-t intervals.
- The bootstrap-$t$ table applies only to the given sample.
- Standard and Student-$t$ distributions are symmetric about zero, thus, the CIs are symmetric about $\hat{\theta}$.
- The bootstrap-$t$ percentiles can be asymmetric about 0, so CI can be longer on the left or right.
    - This property improves the coverage of the bootstrap-$t$ CI.

## Pivotal statistic
- If $$Z = \dfrac{\hat{\theta}-\theta}{\hat{\text{se}}}$$ is called an approximate pivot.
    - The distribution of $Z$ is approximately the same for each value of $\theta$.
- If $Z$ is a pivotal statistic, then the distribution of $Z$ does not depend on any unknown parameters.

## The bootstrap-t interval (Note)
- Bootstrap-$t$ particularly applicable to location statistics (sample mean, median, trimmed mean, sample percentile)
    - location statistic: increasing data value $x_{i}$ by a constant $c$ increases the statistic by $c$. 
- Bootstrap-$t$ may not have the correct coverage with its simple form.
    - For example, CI for correlation coefficient.
- We require computing $\hat{\text{se}}^{*}\left(b \right)$ using bootstrap or jackknife for which there is no simple standard error formula. 
    - For the example, where $\hat{\theta}$ is the sample mean, we use the plug-in estimate of $\hat{\text{se}}^{*}\left(b \right)$ for each bootstrap sample $\vx^{*b}.$
    
## Nested levels of bootstrap sampling (Example)
- Use bootstrap estimate of standard error for each bootstrap sample (two nested levels of bootstrap sampling).
    - Let's choose B = 25 to estimate standard error.
    
## Nested levels of bootstrap sampling (Example)
```{r}
library(magrittr)
B = 1000; B2 = 25
n = length(x); theta.hat = mean(x)
#se.theta.hat = sd(x)/sqrt(n)
bootstrap.results = lapply(as.list(1:B), function(b){
  x.star  = sample(x, size = n, replace = TRUE)
  theta.hat.star = mean(x.star)
  theta.hat.star.star.B2 = lapply(as.list(1:B2), 
    function(bb){
    x.star.star = sample(x.star, 
      size = n, replace = TRUE)
    theta.hat.star.star = mean(x.star.star)
    return(theta.hat.star.star)
  }) %>% unlist
  return(list(theta.hat.star, theta.hat.star.star.B2))
})
```
    
## Nested levels of bootstrap sampling (Example)

$\hat{\theta}^{*b}, b = 1, 2, \cdots, 1000$ and compute $\hat{\text{se}}\left(\hat{\theta}\right)$.
```{r}
theta.hat.star = lapply(bootstrap.results, 
  '[[', 1) %>% unlist
se.theta.hat = sd(theta.hat.star)
se.theta.hat
```
 
## Nested levels of bootstrap sampling (Example)   
- For each $b = 1, 2, \cdots, 1000$, compute $\hat{\text{se}}^{*}\left(b \right)$ using $\hat{\theta}^{**{b2}}, b2 = 1, 2, \cdots, 25$.
```{r}
theta.hat.star.star.B2 = lapply(bootstrap.results, 
  '[[', 2)
se.theta.hat.star = lapply(theta.hat.star.star.B2, 
  sd) %>% unlist
z.star.B = (theta.hat.star 
  - theta.hat)/se.theta.hat.star
```

## Nested levels of bootstrap sampling (Example)
```{r}
alpha = .05
k = ceiling((B+1)*alpha)
t.hat.one.minus.alpha = sort(z.star.B, 
  decreasing = TRUE)[k]
t1 = t.hat.one.minus.alpha; t1
k.u = B+1-k
t.hat.alpha = sort(z.star.B, 
  decreasing = TRUE)[k.u]
t2 = t.hat.alpha; t2
```

## Nested levels of bootstrap sampling (Example)

- 90\% confidence interval for $\theta$ using nested bootstrap

```{r}
ci.bootstrap.t.nested = round(theta.hat - 
    c(t1, t2)*se.theta.hat, 
  digits = 2)
ci.bootstrap.t.nested
```

- Similar to bootstrap-$t$
```{r}
ci.bootstrap.t
```


## Transformation and bootstrap-t (overview)
- Use transformation to overcome issues in bootstrap-t interval in small-sample, nonparametric setting.
- Example of Law school data.
    - Parameter of interest is on correlation coefficient $\theta$ of LSAT and GPA.
```{r}
library(bootstrap)
data(law)
t(law)
```

## Transformation and bootstrap-t (Example)
- Construct CI for $\theta$ without any transformation.
- Use two nested levels bootstrap.
```{r}
B = 1000
B2 = 25
n = dim(law)[1]
theta.hat = cor(law$LSAT, law$GPA)
theta.hat
```

## Transformation and bootstrap-t (Example)
```{r}
bootstrap.results = lapply(as.list(1:B), function(b){
  x = law
  x.star  = x[sample(1:n, size = n, 
    replace = TRUE),]
  theta.hat.star = cor(x.star$LSAT, 
    x.star$GPA)
  theta.hat.star.star.B2 = lapply(as.list(1:B2), 
    function(bb){
    x.star.star = x.star[sample(1:n, size = n, 
      replace = TRUE),]
    theta.hat.star.star = cor(x.star.star$LSAT, 
      x.star.star$GPA)
    return(theta.hat.star.star)
  }) %>% unlist
  return(list(theta.hat.star, 
    theta.hat.star.star.B2))
})

```

## Transformation and bootstrap-t (Example)
- without any transformation
```{r}
theta.hat.star = lapply(bootstrap.results, 
  '[[', 1) %>% unlist
se.theta.hat = sd(theta.hat.star)
se.theta.hat
```

## Transformation and bootstrap-t (Example)
- without any transformation
```{r}
theta.hat.star.star.B2 = lapply(bootstrap.results, 
  '[[', 2)
se.theta.hat.star = lapply(theta.hat.star.star.B2, 
  sd) %>% unlist
z.star.B = (theta.hat.star - 
    theta.hat)/se.theta.hat.star
```

## Transformation and bootstrap-t (Example)
- without any transformation
- 90\% confidence interval for $\theta$ (correlation coefficient)
```{r}
alpha = .05
k = ceiling((B+1)*alpha)
t.hat.one.minus.alpha = sort(z.star.B, 
  decreasing = TRUE)[k]
t1 = t.hat.one.minus.alpha; t1
k.u = B+1-k
t.hat.alpha = sort(z.star.B, 
  decreasing = TRUE)[k.u]
t2 = t.hat.alpha; t2
```

## Transformation and bootstrap-t (Example)
- without any transformation
```{r}
corr.ci.boot.t.no.tran.90 = round(theta.hat - 
    c(t1, t2)*se.theta.hat, 
  digits = 2)
corr.ci.boot.t.no.tran.90
```

## Transformation and bootstrap-t (Example)
- without any transformation
- 98\% confidence interval for $\theta$ (correlation coefficient)
```{r}
alpha = .01
k = ceiling((B+1)*alpha)
t.hat.one.minus.alpha = sort(z.star.B, 
  decreasing = TRUE)[k]
t1 = t.hat.one.minus.alpha; t1
k.u = B+1-k
t.hat.alpha = sort(z.star.B, 
  decreasing = TRUE)[k.u]
t2 = t.hat.alpha; t2
```

## Transformation and bootstrap-t (Example)
- without any transformation
```{r}
corr.ci.boot.t.no.tran.98 = round(theta.hat - 
    c(t1, t2)*se.theta.hat, 
  digits = 2)
corr.ci.boot.t.no.tran.98
```

## Transformation and bootstrap-t 
- \rc Untransformed bootstrap-$t$ \bc procedure may lead intervals which are often too wide and fall outside of allowable range for a parameter.

## Transformation and bootstrap-t (Example)

- Let $$\phi = .5 \text{log}\left( \dfrac{1+\theta}{1-\theta}\right).$$
- Construct CI for $\phi$.
- Transform the endpoints back with the inverse transformation $$\dfrac{\text{e}^{2\phi}-1}{\text{e}^{2\phi}+1}$$ to obtain better interval for $\theta$.

```{r}
phi = function(cor.coeff){
  .5*(log(1 + cor.coeff) - log(1 - cor.coeff))
}
```

## Transformation and bootstrap-t (Example)
- Confidence interval for $\phi$.
```{r}
B = 1000
B2 = 25
n = dim(law)[1]
theta.hat = phi(cor(law$LSAT, law$GPA))
theta.hat
```

## Transformation and bootstrap-t (Example)

- Confidence interval for $\phi$.
```{r}
bootstrap.results = lapply(as.list(1:B), function(b){
  x = law
  x.star  = x[sample(1:n, size = n, 
    replace = TRUE),]
  theta.hat.star = phi(cor(x.star$LSAT, 
    x.star$GPA))
  theta.hat.star.star.B2 = lapply(as.list(1:B2), 
    function(bb){
    x.star.star = x.star[sample(1:n, size = n, 
      replace = TRUE),]
    theta.hat.star.star = phi(cor(x.star.star$LSAT, x.star.star$GPA))
    return(theta.hat.star.star)
  }) %>% unlist
  return(list(theta.hat.star, 
    theta.hat.star.star.B2))
})
```


## Transformation and bootstrap-t (Example)
- Confidence interval for $\phi$.
```{r}
theta.hat.star = lapply(bootstrap.results, 
  '[[', 1) %>% unlist
se.theta.hat = sd(theta.hat.star)
se.theta.hat
theta.hat.star.star.B2 = lapply(bootstrap.results, 
  '[[', 2)
se.theta.hat.star = lapply(theta.hat.star.star.B2, 
  sd) %>% unlist
z.star.B = (theta.hat.star - 
    theta.hat)/se.theta.hat.star
```


## Transformation and bootstrap-t (Example)

- 90\% confidence interval for $\phi$ 
```{r}
alpha = .05
k = ceiling((B+1)*alpha)
t.hat.one.minus.alpha = sort(z.star.B, 
  decreasing = TRUE)[k]
t1 = t.hat.one.minus.alpha; t1
k.u = B+1-k
t.hat.alpha = sort(z.star.B, 
  decreasing = TRUE)[k.u]
t2 = t.hat.alpha; t2
```

## Transformation and bootstrap-t (Example)

- 90\% confidence interval for $\phi$ 

```{r}
ci.phi.bootstrap.t = round(theta.hat - 
    c(t1, t2)*se.theta.hat, 
  digits = 2)
ci.phi.bootstrap.t
```

## Transformation and bootstrap-t (Example)
- 90\% confidence interval for $\theta$ (correlation coefficient)
```{r}
l.phi = round(theta.hat - 
    c(t1, t2)*se.theta.hat, 
  digits = 2)[1]
u.phi = round(theta.hat - 
    c(t1, t2)*se.theta.hat, 
  digits = 2)[2]
corr.ci.boot.t.tran.90 = round(c((exp(2*l.phi)-1)/(exp(2*l.phi)+1),
  (exp(2*u.phi)-1)/(exp(2*u.phi)+1)), digits = 2)
corr.ci.boot.t.tran.90
```

## Transformation and bootstrap-t (Note)
- bootstrap-$t$ depends on the scale - some scales better than others.
- $$\phi = .5 \text{log}\left( \dfrac{1+\theta}{1-\theta}\right)$$ is appropriate when $\left(X, Y\right)$ are bivariate normal.
- What transformation to use?
    - Use bootstrap to estimate the appropriate transformation.
    - We need to variance stablize the estimate $\hat{\theta}$.
        - Make variance of $\hat{\theta}$ is approximately contstant.

## Transformation and bootstrap-t (Choosing the transformation)   
- $X$ is a random variable with mean $\theta$ and standard deviataion $s\left(\theta \right)$. 
- Find a transformation $g$ such that $$g\left( x\right) = \int^{x} \dfrac{1}{s\left(u\right)}du.$$
- Then, variance of $g\left(X\right)$ is constant.
- $s\left(u \right)$ is unknown, but we can write $s\left(u \right) = \text{se}\left(\hat{\theta}|\theta = u \right).$
    - Use bootstrap to estimate $\text{se}\left(\hat{\theta}|\theta = u \right)$.

## Transformation and bootstrap-t (Choosing the transformation) 
- Generate $B = 100$ of $x^{*b}$, compute $\hat{\theta}^{*}\left(b\right)$. 
    - Sample from $x^{*b}$: $R = 25$ bootstrap samples of $x^{**r}$.
    - Compute $\hat{\theta}^{**}\left(r\right)$ and $\hat{\text{se}}\left(\hat{\theta}^{*}\left(b\right) \right)$.
    
## Transformation and bootstrap-t (Choosing the transformation) 
- Fit a curve to the points $\left[\hat{\theta}^{*}\left(b\right),\hspace{.1in} \hat{\text{se}}\left(\hat{\theta}^{*}\left(b\right) \right) \right]$.
![Example](bootstrap_variance_stab.png)

## Transformation and bootstrap-t (Choosing the transformation) 
- Estimate the variance stabilizing transformation $g\left(\hat{\theta} \right)$ - use numerical integration.

![](bootstrap_variance_stab.png)

## Transformation and bootstrap-t (procedure)
- Use $B =1000$ bootstrap samples to construct CI for $\phi = g\left(\theta\right)$ 
    - set the denominator in $\dfrac{g\left(\hat{\theta}^{*} \right)- g\left(\hat{\theta} \right)}{\hat{\text{se}}^{*}}$ to 1.


## Transformation and bootstrap-t (Example) 
- Law school data.
- CI for correlation coefficient $\theta$ between LSAT and GPA.
```{r}
xdata = law %>% as.matrix
n = dim(xdata)[1]
theta = function(x, xdata){
  cor(xdata[x,1], xdata[x,2]) 
  }
results = boott(1:n,theta, xdata, 
  VS = TRUE, perc = c(.01,.05, .95, .99))
```

## Transformation and bootstrap-t (Example) 
- 90\% CI
```{r}
round(c(results$confpoints[2], 
  results$confpoints[3]), digits = 2)
```
- 98\% CI
```{r}
round(c(results$confpoints[1], 
  results$confpoints[4]), digits = 2)
```
- bootstrap-$t$ intervals with transformation are shorter than those without transformation.
- CIs are within the permissible values.
- No need to do nested bootstrap sampling.
- \rc Next, we will work directly with the bootstrap distribution of $\hat{\theta}$ and derive a transformation-respecting confidence interval procedure.

## Bias-corrected and accelerated bootstrap - BCa (Overview)
- BCa interval endpoints are also given by percentile distribution after correction for bias and skewness.
- Recall: percentile method
    - Bootstrap replicates $\hat{\theta}^{*}\left(1\right), \hat{\theta}^{*}\left(2\right), \cdots, \hat{\theta}^{*}\left(B\right)$.
    - The percentile interval $$\left(\hat{\theta}_{lo},  \hat{\theta}_{up}\right) = \left(\hat{\theta}^{\*(\alpha)}, \hat{\theta}^{\*(1-\alpha)} \right),$$ where $\hat{\theta}^{\*(\alpha)}$ is the $100\cdot\alpha$th percentile of $B$ bootstrap replicates.
    
## BCa bootstrap procedure
- Assume that there is a monotone increasing transformation g
such that $$\phi = g\left(\theta \right) \hspace{.1in} \text{and} \hspace{.1in} \hat{\phi} = g\left(\hat{\theta} \right).$$  
- The BCa bootstrap bootstrap is based on the following model $$\dfrac{\hat{\phi} - \phi}{\sigma_{\phi}} \sim \text{N}\left(-z_{0}, 1 \right)\hspace{.1in} \text{with} \hspace{.1in} \sigma_{\phi} = 1+a\phi.$$
- This is a generalization of th usual normal approxiamtion $$\dfrac{\hat{\theta} -\theta}{\text{se}} \sim \text{N}\left(0, 1 \right).$$
    - generalization: transformation $g\left(\cdot \right)$, the bias correction $z_{0}$, and the acceleration $a$.
    
## BCa bootstrap procedure
- The BCa interval of intended coverage $1-2\alpha$, is given by
$$\left(\hat{\theta}_{lo}, \hat{\theta}_{up}\right) = \left(\hat{\theta}^{*(\alpha_{1})}, \hat{\theta}^{*(\alpha_{2})}\right),$$
$$\alpha_{1} = \Phi \left(\hat{z}_{0} + \dfrac{\hat{z}_{0}+ z^{\alpha}}{1- \hat{a}\left(\hat{z}_{0}+ z^{\alpha}\right)}\right)$$ $$\alpha_{2} = \Phi \left(\hat{z}_{0} + \dfrac{\hat{z}_{0}+ z^{1-\alpha}}{1- \hat{a}\left(\hat{z}_{0}+ z^{1-\alpha}\right)}\right)$$ 

## BCa bootstrap procedure
- and $\Phi\left(\cdot \right)$ is the standard normal cumulative distirbution function
- $z^{\alpha}$ - $100\alpha$th percentile point of $\text{N}\left(0,1\right)$.
- $\hat{z}_{0} = \Phi^{-1}\left( \dfrac{\# \lbrace\hat{\theta}^{*}\left(b\right) < \hat{\theta}\rbrace}{B}\right)$, 
    - where $\Phi^{-1}$ is the inverse function of the $\text{N}\left(0,1\right)$.
- $\hat{a} = \dfrac{\sum_{i=1}^{n}\left(\hat{\theta}_{(\cdot)} -\hat{\theta}_{(i)}  \right)^{3}}{6 \left\lbrace \sum_{i=1}^{n}\left(\hat{\theta}_{(\cdot)} -\hat{\theta}_{(i)}  \right)^{2} \right\rbrace^{3/2}}$.
    - Compute the estimate by deleting $i$-th observation, $\hat{\theta}_{(i)}$.
    - $\hat{\theta}_{(\cdot)} = \dfrac{\sum_{i=1}^{n}\hat{\theta}_{(i)} }{n}$.
    
## BCa bootstrap (Example)   
- 90\% CI for correlation between LSAT and GPA (law school data)

```{r}
library(bootstrap)
xdata = law %>% as.matrix()
n = dim(xdata)[1]
theta = function(x, xdata){
  cor(xdata[x,1], xdata[x,2])
}
results = bcanon(1:n, 100, theta, xdata)
corr.ci.bca.90 = c(results$confpoints[2,2],
  results$confpoints[7,2]);
corr.ci.bca.90
```

## Comparison of bootstrap intervals (Example)  

```{r}
corr.ci.boot.t.no.tran.90
```

```{r}
corr.ci.boot.t.tran.90
```

```{r}
round(corr.ci.bca.90, digits = 2)
```

## BCa bootstrap(Note)
- The bootstrap-t method is second-order accurate, but not transformation respecting.
- The percentile method is transformation respecting but not second-order accurate.
- BCa is second-order accurate and transformation respecting.
- We can reduce the computation cost for BCa for **smooth estimates**. 

## The ABC method (Overview)
- The approximate bootstrap confidence intervals
- Approxiamting the BCa interval endpoints analytically - no need Monte Carlo replications.

## The ABC method (Example)
- 90\% CI for correlation between LSAT and GPA (law school data)
```{r}
x = law %>% as.matrix()
theta = function(p, x){
  x1m = sum(p*x[, 1])/sum(p)
  x2m = sum(p*x[, 2])/sum(p)
  numerator = sum(p*(x[, 1]- x1m)*(x[, 2]- x2m))
  denominator = sqrt(sum(p*(x[, 1] - x1m)^2)*sum(p*(x[, 2] - x2m)^2))
  return(numerator/denominator)
}

results = abcnon(x, theta)
round(c(results$limits[2,2], results$limits[7,2]), 2)
```


##  References for this lecture

*ET* Chapter 12.